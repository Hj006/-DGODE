{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d545e30-cb76-4092-8e69-26df483d86d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.40.1\n",
      "  Downloading transformers-4.40.1-py3-none-any.whl.metadata (137 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting soundfile\n",
      "  Downloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl.metadata (16 kB)\n",
      "Collecting ftfy\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting regex\n",
      "  Downloading regex-2025.10.23-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting einops\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.65.0)\n",
      "Collecting librosa\n",
      "  Downloading librosa-0.11.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting timm\n",
      "  Downloading timm-1.0.21-py3-none-any.whl.metadata (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting opencv-python-headless\n",
      "  Downloading opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
      "Collecting ffmpeg-python\n",
      "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting py7zr\n",
      "  Downloading py7zr-1.0.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.1) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers==4.40.1)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.1) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.1) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.1) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.1) (2.31.0)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers==4.40.1)\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers==4.40.1)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.10/site-packages (from soundfile) (1.16.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from ftfy) (0.2.5)\n",
      "Collecting audioread>=2.1.9 (from librosa)\n",
      "  Downloading audioread-3.1.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting numba>=0.51.0 (from librosa)\n",
      "  Downloading numba-0.62.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
      "Collecting scikit-learn>=1.1.0 (from librosa)\n",
      "  Downloading scikit_learn-1.7.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting joblib>=1.0 (from librosa)\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (5.1.1)\n",
      "Collecting pooch>=1.1 (from librosa)\n",
      "  Downloading pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting soxr>=0.3.2 (from librosa)\n",
      "  Downloading soxr-1.0.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in /opt/conda/lib/python3.10/site-packages (from librosa) (4.9.0)\n",
      "Collecting lazy_loader>=0.1 (from librosa)\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting msgpack>=1.0 (from librosa)\n",
      "  Downloading msgpack-1.1.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from timm) (2.2.0)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from timm) (0.17.0)\n",
      "Collecting numpy>=1.17 (from transformers==4.40.1)\n",
      "  Downloading numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting future (from ffmpeg-python)\n",
      "  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting texttable (from py7zr)\n",
      "  Downloading texttable-1.7.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting pycryptodomex>=3.20.0 (from py7zr)\n",
      "  Downloading pycryptodomex-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting brotli>=1.1.0 (from py7zr)\n",
      "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from py7zr) (5.9.0)\n",
      "Collecting pyzstd>=0.16.1 (from py7zr)\n",
      "  Downloading pyzstd-0.18.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.6 kB)\n",
      "Collecting pyppmd<1.3.0,>=1.1.0 (from py7zr)\n",
      "  Downloading pyppmd-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting pybcj<1.1.0,>=1.0.0 (from py7zr)\n",
      "  Downloading pybcj-1.0.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
      "Collecting multivolumefile>=0.2.3 (from py7zr)\n",
      "  Downloading multivolumefile-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting inflate64<1.1.0,>=1.0.0 (from py7zr)\n",
      "  Downloading inflate64-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0->soundfile) (2.21)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.1) (2023.12.2)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.1)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting llvmlite<0.46,>=0.45.0dev0 (from numba>=0.51.0->librosa)\n",
      "  Downloading llvmlite-0.45.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.1->librosa) (3.10.0)\n",
      "Collecting typing_extensions>=4.1.1 (from librosa)\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.1) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.1) (2023.11.17)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=1.1.0->librosa)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->timm) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->timm) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->timm) (3.1.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->timm) (10.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->timm) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->timm) (1.3.0)\n",
      "Downloading transformers-4.40.1-py3-none-any.whl (9.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m132.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.10.23-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (791 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m791.5/791.5 kB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading librosa-0.11.0-py3-none-any.whl (260 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.7/260.7 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading timm-1.0.21-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m100.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (54.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
      "Downloading py7zr-1.0.0-py3-none-any.whl (69 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading audioread-3.1.0-py3-none-any.whl (23 kB)\n",
      "Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m117.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading inflate64-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.5/93.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.4/308.4 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading msgpack-1.1.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (406 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m406.1/406.1 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
      "Downloading numba-0.62.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m113.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m110.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.6/64.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pybcj-1.0.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pycryptodomex-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyppmd-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.0/139.0 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyzstd-0.18.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (428 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m428.8/428.8 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.8/485.8 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.7.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m143.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading soxr-1.0.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (242 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.3/242.3 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m115.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading future-1.0.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.3/491.3 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m118.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llvmlite-0.45.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: texttable, brotli, typing_extensions, threadpoolctl, safetensors, regex, pyppmd, pycryptodomex, pybcj, numpy, multivolumefile, msgpack, llvmlite, lazy_loader, joblib, inflate64, hf-xet, future, ftfy, einops, audioread, soxr, soundfile, scipy, pyzstd, pooch, opencv-python-headless, numba, huggingface-hub, ffmpeg-python, tokenizers, scikit-learn, py7zr, transformers, timm, librosa\n",
      "  Attempting uninstall: brotli\n",
      "    Found existing installation: Brotli 1.0.9\n",
      "    Uninstalling Brotli-1.0.9:\n",
      "      Successfully uninstalled Brotli-1.0.9\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.9.0\n",
      "    Uninstalling typing_extensions-4.9.0:\n",
      "      Successfully uninstalled typing_extensions-4.9.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.3\n",
      "    Uninstalling numpy-1.26.3:\n",
      "      Successfully uninstalled numpy-1.26.3\n",
      "Successfully installed audioread-3.1.0 brotli-1.1.0 einops-0.8.1 ffmpeg-python-0.2.0 ftfy-6.3.1 future-1.0.0 hf-xet-1.2.0 huggingface-hub-0.36.0 inflate64-1.0.3 joblib-1.5.2 lazy_loader-0.4 librosa-0.11.0 llvmlite-0.45.1 msgpack-1.1.2 multivolumefile-0.2.3 numba-0.62.1 numpy-2.2.6 opencv-python-headless-4.12.0.88 pooch-1.8.2 py7zr-1.0.0 pybcj-1.0.6 pycryptodomex-3.23.0 pyppmd-1.2.0 pyzstd-0.18.0 regex-2025.10.23 safetensors-0.6.2 scikit-learn-1.7.2 scipy-1.15.3 soundfile-0.13.1 soxr-1.0.0 texttable-1.7.0 threadpoolctl-3.6.0 timm-1.0.21 tokenizers-0.19.1 transformers-4.40.1 typing_extensions-4.15.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install soundfile ftfy regex einops scipy tqdm librosa timm opencv-python-headless ffmpeg-python py7zr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "babb1973-90f6-4e02-9a7d-24272f0cd3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorchvideo\n",
      "  Downloading pytorchvideo-0.1.5.tar.gz (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting fvcore (from pytorchvideo)\n",
      "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting av (from pytorchvideo)\n",
      "  Downloading av-16.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
      "Collecting parameterized (from pytorchvideo)\n",
      "  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
      "Collecting iopath (from pytorchvideo)\n",
      "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from pytorchvideo) (3.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from fvcore->pytorchvideo) (2.2.6)\n",
      "Collecting yacs>=0.1.6 (from fvcore->pytorchvideo)\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from fvcore->pytorchvideo) (6.0.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from fvcore->pytorchvideo) (4.65.0)\n",
      "Collecting termcolor>=1.1 (from fvcore->pytorchvideo)\n",
      "  Downloading termcolor-3.2.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from fvcore->pytorchvideo) (10.0.1)\n",
      "Collecting tabulate (from fvcore->pytorchvideo)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.10/site-packages (from iopath->pytorchvideo) (4.15.0)\n",
      "Collecting portalocker (from iopath->pytorchvideo)\n",
      "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Downloading av-16.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (39.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
      "Downloading termcolor-3.2.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Building wheels for collected packages: pytorchvideo, fvcore, iopath\n",
      "  Building wheel for pytorchvideo (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pytorchvideo: filename=pytorchvideo-0.1.5-py3-none-any.whl size=188685 sha256=adc6b1ed452f0f0853977c19b9a01f2572d87f67a28bc7d1eae624620929d099\n",
      "  Stored in directory: /root/.cache/pip/wheels/ff/4e/81/0f72a543be9ed7eb737c95bfc5da4025e73226b44368074ece\n",
      "  Building wheel for fvcore (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61400 sha256=eb739044fcc50ba3f3fe1a43f263ceb08dd32b6c3bdd423fae7b1fcff07e301d\n",
      "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
      "  Building wheel for iopath (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31530 sha256=a718201d72c8ca284c713d52f365d18b807c560f9b379034c33e25b2b1010e77\n",
      "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
      "Successfully built pytorchvideo fvcore iopath\n",
      "Installing collected packages: yacs, termcolor, tabulate, portalocker, parameterized, av, iopath, fvcore, pytorchvideo\n",
      "Successfully installed av-16.0.1 fvcore-0.1.5.post20221221 iopath-0.1.10 parameterized-0.9.0 portalocker-3.2.0 pytorchvideo-0.1.5 tabulate-0.9.0 termcolor-3.2.0 yacs-0.1.8\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pytorchvideo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7735c406-ffaf-4bee-b9d5-1ccbb7446e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] missing segment for Ses05M_impro06_FXX2\n",
      "Parsed 2195 utterances in total.\n",
      " Metadata saved to: /workspace/dataset/S5/meta/train_meta.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "root_dir = \"/workspace/dataset/S5\"\n",
    "txt_dir = os.path.join(root_dir, \"TXT\")\n",
    "\n",
    "\n",
    "pattern = re.compile(r\"(\\S+)\\s*\\[(\\d+\\.\\d+)-(\\d+\\.\\d+)\\]:\\s*(.*)\")\n",
    "\n",
    "dataset = []\n",
    "\n",
    "for fname in os.listdir(txt_dir):\n",
    "    if not fname.endswith(\".txt\"):\n",
    "        continue\n",
    "\n",
    "    video_name = os.path.splitext(fname)[0]\n",
    "    txt_path = os.path.join(txt_dir, fname)\n",
    "    video_dir = os.path.join(root_dir, video_name)\n",
    "\n",
    "    if not os.path.exists(video_dir):\n",
    "        print(f\"[WARN] missing video folder for {video_name}\")\n",
    "        continue\n",
    "\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        m = pattern.match(line)\n",
    "        if not m:\n",
    "            continue\n",
    "\n",
    "        utt_id, start, end, text = m.groups()\n",
    "        video_path = os.path.join(video_dir, f\"{utt_id}.mp4\")\n",
    "        audio_path = os.path.join(video_dir, f\"{utt_id}.wav\")\n",
    "\n",
    "        if not (os.path.exists(video_path) and os.path.exists(audio_path)):\n",
    "            print(f\"[WARN] missing segment for {utt_id}\")\n",
    "            continue\n",
    "\n",
    "        dataset.append({\n",
    "            \"utt_id\": utt_id,\n",
    "            \"video_path\": video_path,\n",
    "            \"audio_path\": audio_path,\n",
    "            \"text\": text.strip(),\n",
    "            \"start_time\": float(start),\n",
    "            \"end_time\": float(end)\n",
    "        })\n",
    "\n",
    "print(f\"Parsed {len(dataset)} utterances in total.\")\n",
    "os.makedirs(os.path.join(root_dir, \"meta\"), exist_ok=True)\n",
    "\n",
    "out_json = os.path.join(root_dir, \"meta\", \"train_meta.json\")\n",
    "with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(dataset, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\" Metadata saved to: {out_json}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c28c2e5-91fc-446e-ad25-8b4e6bc11245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.4.0+cu121\n",
      "Uninstalling torch-2.4.0+cu121:\n",
      "  Successfully uninstalled torch-2.4.0+cu121\n",
      "Found existing installation: torchvision 0.19.0+cu121\n",
      "Uninstalling torchvision-0.19.0+cu121:\n",
      "  Successfully uninstalled torchvision-0.19.0+cu121\n",
      "Found existing installation: torchaudio 2.4.0+cu121\n",
      "Uninstalling torchaudio-2.4.0+cu121:\n",
      "  Successfully uninstalled torchaudio-2.4.0+cu121\n",
      "Found existing installation: pytorchvideo 0.1.5\n",
      "Uninstalling pytorchvideo-0.1.5:\n",
      "  Successfully uninstalled pytorchvideo-0.1.5\n",
      "\u001b[33mWARNING: Skipping transformers as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping accelerate as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting torch==2.1.2\n",
      "  Using cached torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Collecting torchvision==0.16.2\n",
      "  Using cached torchvision-0.16.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting torchaudio==2.1.2\n",
      "  Using cached torchaudio-2.1.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting pytorchvideo==0.1.5\n",
      "  Using cached pytorchvideo-0.1.5-py3-none-any.whl\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2) (4.15.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2) (12.1.105)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.1.2)\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2) (12.1.0.106)\n",
      "Collecting nvidia-nccl-cu12==2.18.1 (from torch==2.1.2)\n",
      "  Using cached nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2) (12.1.105)\n",
      "Collecting triton==2.1.0 (from torch==2.1.2)\n",
      "  Using cached triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision==0.16.2) (2.2.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision==0.16.2) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision==0.16.2) (12.0.0)\n",
      "Requirement already satisfied: fvcore in /opt/conda/lib/python3.10/site-packages (from pytorchvideo==0.1.5) (0.1.5.post20221221)\n",
      "Requirement already satisfied: av in /opt/conda/lib/python3.10/site-packages (from pytorchvideo==0.1.5) (16.0.1)\n",
      "Requirement already satisfied: parameterized in /opt/conda/lib/python3.10/site-packages (from pytorchvideo==0.1.5) (0.9.0)\n",
      "Requirement already satisfied: iopath in /opt/conda/lib/python3.10/site-packages (from pytorchvideo==0.1.5) (0.1.10)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.2) (12.9.86)\n",
      "Requirement already satisfied: yacs>=0.1.6 in /opt/conda/lib/python3.10/site-packages (from fvcore->pytorchvideo==0.1.5) (0.1.8)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from fvcore->pytorchvideo==0.1.5) (6.0.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from fvcore->pytorchvideo==0.1.5) (4.65.0)\n",
      "Requirement already satisfied: termcolor>=1.1 in /opt/conda/lib/python3.10/site-packages (from fvcore->pytorchvideo==0.1.5) (3.2.0)\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from fvcore->pytorchvideo==0.1.5) (0.9.0)\n",
      "Requirement already satisfied: portalocker in /opt/conda/lib/python3.10/site-packages (from iopath->pytorchvideo==0.1.5) (3.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.1.2) (3.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.16.2) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.16.2) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.16.2) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.16.2) (2023.11.17)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.1.2) (1.3.0)\n",
      "Using cached torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
      "Using cached torchvision-0.16.2-cp310-cp310-manylinux1_x86_64.whl (6.8 MB)\n",
      "Using cached torchaudio-2.1.2-cp310-cp310-manylinux1_x86_64.whl (3.3 MB)\n",
      "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Using cached nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
      "Using cached triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
      "Installing collected packages: triton, nvidia-nccl-cu12, nvidia-cudnn-cu12, torch, pytorchvideo, torchvision, torchaudio\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.0.0\n",
      "    Uninstalling triton-3.0.0:\n",
      "      Successfully uninstalled triton-3.0.0\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.20.5\n",
      "    Uninstalling nvidia-nccl-cu12-2.20.5:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.20.5\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n",
      "    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n",
      "Successfully installed nvidia-cudnn-cu12-8.9.2.26 nvidia-nccl-cu12-2.18.1 pytorchvideo-0.1.5 torch-2.1.2 torchaudio-2.1.2 torchvision-0.16.2 triton-2.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y torch torchvision torchaudio pytorchvideo transformers accelerate\n",
    "\n",
    "!pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 pytorchvideo==0.1.5\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80f0d9e3-b66c-4067-bf16-3c2e3cc662e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2+cu121 0.16.2+cu121 2.1.2+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch, torchvision, torchaudio; print(torch.__version__, torchvision.__version__, torchaudio.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c365241-36f5-4281-9b2c-7bcf12fc097f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n image bind 的定义\\ndef load_and_transform_vision_data(image_paths, device):\\n    if image_paths is None:\\n        return None\\n\\n    image_outputs = []\\n\\n    data_transform = transforms.Compose(\\n        [\\n            transforms.Resize(\\n                224, interpolation=transforms.InterpolationMode.BICUBIC\\n            ),\\n            transforms.CenterCrop(224),\\n            transforms.ToTensor(),\\n            transforms.Normalize(\\n                mean=(0.48145466, 0.4578275, 0.40821073),\\n                std=(0.26862954, 0.26130258, 0.27577711),\\n            ),\\n        ]\\n    )\\n    \\n    for image_path in image_paths:\\n        with open(image_path, \"rb\") as fopen:\\n            image = Image.open(fopen).convert(\"RGB\")\\n\\n        image = data_transform(image).to(device)\\n        image_outputs.append(image)\\n    return torch.stack(image_outputs, dim=0)\\n    \\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    " image bind 的定义\n",
    "def load_and_transform_vision_data(image_paths, device):\n",
    "    if image_paths is None:\n",
    "        return None\n",
    "\n",
    "    image_outputs = []\n",
    "\n",
    "    data_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(\n",
    "                224, interpolation=transforms.InterpolationMode.BICUBIC\n",
    "            ),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=(0.48145466, 0.4578275, 0.40821073),\n",
    "                std=(0.26862954, 0.26130258, 0.27577711),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    for image_path in image_paths:\n",
    "        with open(image_path, \"rb\") as fopen:\n",
    "            image = Image.open(fopen).convert(\"RGB\")\n",
    "\n",
    "        image = data_transform(image).to(device)\n",
    "        image_outputs.append(image)\n",
    "    return torch.stack(image_outputs, dim=0)\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df3b7ee4-a5a4-492f-9f39-416ed08f7137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting decord\n",
      "  Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl.metadata (422 bytes)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from decord) (2.2.6)\n",
      "Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m96.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: decord\n",
      "Successfully installed decord-0.6.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install decord\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1dde02de-1a29-426f-8fcb-bf299d03700e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (2.2.6)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8f76472-8ac3-4e33-ba84-d206c888912a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12.1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.version.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77cf374e-1032-450d-bb13-c98da31344e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.26.4\n",
      "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.6\n",
      "    Uninstalling numpy-2.2.6:\n",
      "      Successfully uninstalled numpy-2.2.6\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy==1.26.4 --force-reinstall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2ec5e01-2c4c-4062-8622-ae5bb3879e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.1.2+cu121\n",
      "Torchvision: 0.16.2+cu121\n",
      "NumPy: 1.26.4\n",
      "Tensor to numpy: [[0.15420312 0.39508873 0.0564419 ]\n",
      " [0.42654216 0.29004008 0.9550649 ]\n",
      " [0.37730187 0.04210186 0.43736172]]\n"
     ]
    }
   ],
   "source": [
    "import torch, numpy as np, torchvision\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Torchvision:\", torchvision.__version__)\n",
    "print(\"NumPy:\", np.__version__)\n",
    "\n",
    "x = torch.rand(3,3)\n",
    "print(\"Tensor to numpy:\", x.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f81ff4b9-ac3c-47be-bbd8-ee7a98675234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import os\n",
    "print(os.path.exists(os.path.join(os.path.dirname(torchvision.__file__), \"transforms\", \"functional_tensor.py\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e34226-f651-49c3-85ad-1ba50e900632",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torchvision==0.17.2 --force-reinstall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2334783f-329a-4116-9f60-16c11b56ca61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu121\n",
      "Collecting torchvision==0.19.0\n",
      "  Using cached https://download.pytorch.org/whl/cu121/torchvision-0.19.0%2Bcu121-cp310-cp310-linux_x86_64.whl (7.1 MB)\n",
      "Collecting numpy (from torchvision==0.19.0)\n",
      "  Using cached numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting torch==2.4.0 (from torchvision==0.19.0)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/torch-2.4.0%2Bcu121-cp310-cp310-linux_x86_64.whl (799.1 MB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision==0.19.0)\n",
      "  Downloading pillow-12.0.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.8 kB)\n",
      "Collecting filelock (from torch==2.4.0->torchvision==0.19.0)\n",
      "  Downloading filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting typing-extensions>=4.8.0 (from torch==2.4.0->torchvision==0.19.0)\n",
      "  Downloading https://download.pytorch.org/whl/typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting sympy (from torch==2.4.0->torchvision==0.19.0)\n",
      "  Using cached https://download.pytorch.org/whl/sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch==2.4.0->torchvision==0.19.0)\n",
      "  Downloading https://download.pytorch.org/whl/networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch==2.4.0->torchvision==0.19.0)\n",
      "  Downloading https://download.pytorch.org/whl/jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec (from torch==2.4.0->torchvision==0.19.0)\n",
      "  Downloading fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.0->torchvision==0.19.0)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.0->torchvision==0.19.0)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.0->torchvision==0.19.0)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.0->torchvision==0.19.0)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.0->torchvision==0.19.0)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.0->torchvision==0.19.0)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.0->torchvision==0.19.0)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.0->torchvision==0.19.0)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.0->torchvision==0.19.0)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.0->torchvision==0.19.0)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.0->torchvision==0.19.0)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Collecting triton==3.0.0 (from torch==2.4.0->torchvision==0.19.0)\n",
      "  Using cached https://download.pytorch.org/whl/triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0->torchvision==0.19.0)\n",
      "  Downloading https://download.pytorch.org/whl/nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch==2.4.0->torchvision==0.19.0)\n",
      "  Downloading markupsafe-3.0.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)\n",
      "INFO: pip is looking at multiple versions of networkx to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting networkx (from torch==2.4.0->torchvision==0.19.0)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch==2.4.0->torchvision==0.19.0)\n",
      "  Downloading https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-12.0.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hUsing cached numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "Downloading https://download.pytorch.org/whl/typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.0/201.0 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached https://download.pytorch.org/whl/sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Downloading markupsafe-3.0.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (20 kB)\n",
      "Downloading https://download.pytorch.org/whl/nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m96.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, typing-extensions, sympy, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, fsspec, filelock, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchvision\n",
      "  Attempting uninstall: mpmath\n",
      "    Found existing installation: mpmath 1.3.0\n",
      "    Uninstalling mpmath-1.3.0:\n",
      "      Successfully uninstalled mpmath-1.3.0\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.15.0\n",
      "    Uninstalling typing_extensions-4.15.0:\n",
      "      Successfully uninstalled typing_extensions-4.15.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.14.0\n",
      "    Uninstalling sympy-1.14.0:\n",
      "      Successfully uninstalled sympy-1.14.0\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: Pillow 10.0.1\n",
      "    Uninstalling Pillow-10.0.1:\n",
      "      Successfully uninstalled Pillow-10.0.1\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.1.105\n",
      "    Uninstalling nvidia-nvtx-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.20.5\n",
      "    Uninstalling nvidia-nccl-cu12-2.20.5:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.20.5\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.2.106\n",
      "    Uninstalling nvidia-curand-cu12-10.3.2.106:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.2.106\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.0.2.54\n",
      "    Uninstalling nvidia-cufft-cu12-11.0.2.54:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.0.2.54\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.1.105\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.1.105\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.1.105\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.1.3.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.1.3.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.1.3.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: networkx\n",
      "    Found existing installation: networkx 3.1\n",
      "    Uninstalling networkx-3.1:\n",
      "      Successfully uninstalled networkx-3.1\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 2.1.3\n",
      "    Uninstalling MarkupSafe-2.1.3:\n",
      "      Successfully uninstalled MarkupSafe-2.1.3\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.12.2\n",
      "    Uninstalling fsspec-2023.12.2:\n",
      "      Successfully uninstalled fsspec-2023.12.2\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.13.1\n",
      "    Uninstalling filelock-3.13.1:\n",
      "      Successfully uninstalled filelock-3.13.1\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.0.0\n",
      "    Uninstalling triton-3.0.0:\n",
      "      Successfully uninstalled triton-3.0.0\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.1.0.106\n",
      "    Uninstalling nvidia-cusparse-cu12-12.1.0.106:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.1.0.106\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n",
      "    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 3.1.2\n",
      "    Uninstalling Jinja2-3.1.2:\n",
      "      Successfully uninstalled Jinja2-3.1.2\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.4.5.107\n",
      "    Uninstalling nvidia-cusolver-cu12-11.4.5.107:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.4.5.107\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.4.0+cu121\n",
      "    Uninstalling torch-2.4.0+cu121:\n",
      "      Successfully uninstalled torch-2.4.0+cu121\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.19.0+cu121\n",
      "    Uninstalling torchvision-0.19.0+cu121:\n",
      "      Successfully uninstalled torchvision-0.19.0+cu121\n",
      "Successfully installed MarkupSafe-3.0.3 filelock-3.20.0 fsspec-2025.10.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.2.1 numpy-2.2.6 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 pillow-12.0.0 sympy-1.14.0 torch-2.4.0+cu121 torchvision-0.19.0+cu121 triton-3.0.0 typing-extensions-4.15.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --force-reinstall torchvision==0.19.0 --extra-index-url https://download.pytorch.org/whl/cu121\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf76a252-23b2-42f7-b2f9-e0bfdf8bff1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing Ses01F_impro03 ...\n",
      " Saved /workspace/dataset/S1/features/Ses01F_impro03.pt (52 utterances)\n",
      " Processing Ses01F_impro01 ...\n",
      " Saved /workspace/dataset/S1/features/Ses01F_impro01.pt (30 utterances)\n",
      " Processing Ses01F_impro02 ...\n",
      " Saved /workspace/dataset/S1/features/Ses01F_impro02.pt (38 utterances)\n",
      " All videos processed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "from torchvision.transforms import Compose, Lambda\n",
    "from pytorchvideo.transforms import UniformTemporalSubsample\n",
    "from imagebind import data\n",
    "from imagebind.models import imagebind_model\n",
    "from imagebind.models.imagebind_model import ModalityType\n",
    "import torchvision.transforms as T\n",
    "device = \"cuda:0\"\n",
    "model = imagebind_model.imagebind_huge(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False \n",
    "model.eval().to(device)\n",
    "\n",
    "meta_path = \"/workspace/dataset/S1/meta/train_meta.json\"\n",
    "save_dir = \"/workspace/dataset/S1/features\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def load_and_transform_vision_data_from_tensor(video_tensor, device):\n",
    "    \"\"\"\n",
    "    模仿 imagebind.data.load_and_transform_vision_data 的行为，\n",
    "    对视频帧张量做标准化预处理后送入模型。\n",
    "    video_tensor: torch.Tensor [C, T, H, W]\n",
    "    \"\"\"\n",
    "    # ImageBind vision 模块的标准归一化参数\n",
    "    mean = torch.tensor([0.48145466, 0.4578275, 0.40821073], device=device)[:, None, None, None]\n",
    "    std = torch.tensor([0.26862954, 0.26130258, 0.27577711], device=device)[:, None, None, None]\n",
    "\n",
    "    # Resize + CenterCrop（保持一致性）\n",
    "    transform = T.Compose([\n",
    "        T.Resize((224, 224)),\n",
    "    ])\n",
    "\n",
    "    # video_tensor: (C, T, H, W)\n",
    "    frames = []\n",
    "    for t in range(video_tensor.shape[1]):\n",
    "        frame = video_tensor[:, t, :, :].cpu()\n",
    "        frame = T.ToPILImage()(frame)\n",
    "        frame = transform(frame)\n",
    "        frame = T.ToTensor()(frame)\n",
    "        frames.append(frame)\n",
    "\n",
    "    x = torch.stack(frames, dim=0).to(device)  # [T, 3, 224, 224]\n",
    "    x = (x - mean.permute(1, 0, 2, 3)) / std.permute(1, 0, 2, 3)  # 归一化\n",
    "    return x\n",
    "\n",
    "\"\"\"\n",
    "def load_video_gpu(video_path, num_frames=16, max_duration=15.0):\n",
    "    \n",
    "    #用 PyTorchVideo 直接在 GPU 上读取视频并采样为若干帧（不落地jpg）\n",
    "    \n",
    "    video = EncodedVideo.from_path(video_path)\n",
    "    duration = video.duration\n",
    "\n",
    "    # 截断长视频\n",
    "    end_sec = min(duration, max_duration)\n",
    "    clip = video.get_clip(start_sec=0, end_sec=end_sec)\n",
    "\n",
    "    video_data = clip[\"video\"]  # (C, T, H, W)\n",
    "    T = video_data.shape[1]\n",
    "    if T > num_frames:\n",
    "        sampler = UniformTemporalSubsample(num_frames)\n",
    "        video_data = sampler(video_data)\n",
    "    elif T < num_frames:\n",
    "        repeat = (num_frames // T) + 1\n",
    "        video_data = video_data.repeat(1, repeat, 1, 1)[:, :num_frames]\n",
    "\n",
    "    return video_data  # (3, num_frames, H, W)\n",
    "\"\"\"\n",
    "\n",
    "def load_video_gpu(video_path, fps=8, max_duration=15.0):\n",
    "    # 用 decord，只解视频帧，不读音频\n",
    "    video = EncodedVideo.from_path(video_path, decoder=\"decord\", decode_audio=False)\n",
    "\n",
    "    duration = getattr(video, \"duration\", None)\n",
    "    if duration is None or duration <= 0:\n",
    "        print(f\"[WARN] Invalid duration for {video_path}, skipping.\")\n",
    "        return None\n",
    "\n",
    "    duration = min(duration, max_duration)\n",
    "    clip = video.get_clip(start_sec=0, end_sec=duration)\n",
    "\n",
    "    if clip is None or clip.get(\"video\") is None:\n",
    "        print(f\"[WARN] Failed to read video from {video_path}\")\n",
    "        return None\n",
    "\n",
    "    video_data = clip[\"video\"]\n",
    "    if video_data is None:\n",
    "        print(f\"[WARN] Empty video data for {video_path}\")\n",
    "        return None\n",
    "\n",
    "    # 目标帧数 = fps * duration\n",
    "    target_frames = int(math.ceil(fps * duration))\n",
    "    T = video_data.shape[1]\n",
    "\n",
    "    if T > target_frames:\n",
    "        indices = torch.linspace(0, T - 1, target_frames).long()\n",
    "        video_data = video_data[:, indices, :, :]\n",
    "    elif T < target_frames and T > 0:\n",
    "        repeat = (target_frames // T) + 1\n",
    "        video_data = video_data.repeat(1, repeat, 1, 1)[:, :target_frames]\n",
    "\n",
    "    return video_data\n",
    "\n",
    "\n",
    "\n",
    "def extract_embeddings(meta_file):\n",
    "    with open(meta_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        items = json.load(f)\n",
    "\n",
    "    grouped = {}\n",
    "    for item in items:\n",
    "        vid = item[\"video_path\"].split(\"/\")[-2]\n",
    "        grouped.setdefault(vid, []).append(item)\n",
    "\n",
    "    for vid, utterances in grouped.items():\n",
    "        print(f\" Processing {vid} ...\")\n",
    "        all_embeddings = {}\n",
    "\n",
    "        for item in utterances:\n",
    "            utt_id = item[\"utt_id\"]\n",
    "            text = item[\"text\"]\n",
    "            audio_path = item[\"audio_path\"]\n",
    "            video_path = item[\"video_path\"]\n",
    "\n",
    "            # 加载起止时间（可选字段）\n",
    "            start_time = item.get(\"start_time\", None)\n",
    "            end_time = item.get(\"end_time\", None)\n",
    "\n",
    "            # 加载视频\n",
    "            video_tensor = load_video_gpu(video_path)\n",
    "            vision_input = load_and_transform_vision_data_from_tensor(video_tensor, device)\n",
    "\n",
    "            # 文本 & 音频\n",
    "            text_input = data.load_and_transform_text([text], device)\n",
    "            audio_input = data.load_and_transform_audio_data([audio_path], device)\n",
    "\n",
    "            inputs = {\n",
    "                ModalityType.TEXT: text_input,\n",
    "                ModalityType.AUDIO: audio_input,\n",
    "                ModalityType.VISION: vision_input,\n",
    "            }\n",
    "\n",
    "            with torch.no_grad():\n",
    "                emb = model(inputs)\n",
    "\n",
    "            #  保存 embedding + 时间信息\n",
    "            all_embeddings[utt_id] = {\n",
    "                \"text_emb\": emb[ModalityType.TEXT].cpu(),\n",
    "                \"audio_emb\": emb[ModalityType.AUDIO].cpu(),\n",
    "                \"vision_emb\": emb[ModalityType.VISION].mean(dim=0, keepdim=True).cpu(),\n",
    "                \"start_time\": start_time,\n",
    "                \"end_time\": end_time,\n",
    "            }\n",
    "\n",
    "        # 保存当前视频的所有 embedding\n",
    "        save_path = os.path.join(save_dir, f\"{vid}.pt\")\n",
    "        torch.save(all_embeddings, save_path)\n",
    "        print(f\" Saved {save_path} ({len(all_embeddings)} utterances)\")\n",
    "\n",
    "    print(\" All videos processed!\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    extract_embeddings(meta_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59de4431-cb4e-49a2-b6c6-6cc889091d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "['Ses01F_impro01_F000', 'Ses01F_impro01_M000', 'Ses01F_impro01_F001']\n",
      "torch.Size([1, 1024])\n",
      "torch.Size([1, 1024])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "feature_dir = \"/workspace/dataset/S111/features\"\n",
    "\n",
    "# 读取某个 session 的 embeddings\n",
    "path = os.path.join(feature_dir, \"Ses01F_impro01.pt\")\n",
    "data = torch.load(path)\n",
    "\n",
    "print(len(data))  # 一共有多少个utterance\n",
    "print(list(data.keys())[:3])  # 看前三个id\n",
    "\n",
    "utt_id = \"Ses01F_impro01_F000\"\n",
    "print(data[utt_id][\"text_emb\"].shape)\n",
    "print(data[utt_id][\"vision_emb\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "551f56a2-f56c-48fd-aa23-3790afe4fa91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing S1 ===\n",
      " Processing Ses01F_impro01 ...\n",
      "Saved /workspace/dataset/S1/features/Ses01F_impro01.pt (30 utterances)\n",
      " Processing Ses01F_impro02 ...\n",
      "Saved /workspace/dataset/S1/features/Ses01F_impro02.pt (38 utterances)\n",
      " Processing Ses01F_impro03 ...\n",
      "Saved /workspace/dataset/S1/features/Ses01F_impro03.pt (52 utterances)\n",
      " Processing Ses01F_impro04 ...\n",
      "Saved /workspace/dataset/S1/features/Ses01F_impro04.pt (71 utterances)\n",
      " Processing Ses01F_impro05 ...\n",
      "Saved /workspace/dataset/S1/features/Ses01F_impro05.pt (67 utterances)\n",
      " Processing Ses01F_impro06 ...\n",
      "Saved /workspace/dataset/S1/features/Ses01F_impro06.pt (47 utterances)\n",
      " Processing Ses01F_impro07 ...\n",
      "Saved /workspace/dataset/S1/features/Ses01F_impro07.pt (37 utterances)\n",
      " Processing Ses01F_script01_1 ...\n",
      "Saved /workspace/dataset/S1/features/Ses01F_script01_1.pt (89 utterances)\n",
      " Processing Ses01F_script01_2 ...\n",
      "Saved /workspace/dataset/S1/features/Ses01F_script01_2.pt (33 utterances)\n",
      " Processing Ses01F_script01_3 ...\n",
      "Saved /workspace/dataset/S1/features/Ses01F_script01_3.pt (75 utterances)\n",
      " Processing Ses01F_script02_2 ...\n",
      "Saved /workspace/dataset/S1/features/Ses01F_script02_2.pt (98 utterances)\n",
      " Processing Ses01F_script02_1 ...\n",
      "Saved /workspace/dataset/S1/features/Ses01F_script02_1.pt (71 utterances)\n",
      " Processing Ses01F_script03_1 ...\n",
      "Saved /workspace/dataset/S1/features/Ses01F_script03_1.pt (71 utterances)\n",
      " Processing Ses01F_script03_2 ...\n",
      "Saved /workspace/dataset/S1/features/Ses01F_script03_2.pt (82 utterances)\n",
      " Processing Ses01M_impro05 ...\n",
      "Saved /workspace/dataset/S1/features/Ses01M_impro05.pt (70 utterances)\n",
      " Processing Ses01M_impro02 ...\n",
      "Saved /workspace/dataset/S1/features/Ses01M_impro02.pt (48 utterances)\n",
      " Processing Ses01M_impro03 ...\n",
      "Saved /workspace/dataset/S1/features/Ses01M_impro03.pt (53 utterances)\n",
      " Processing Ses01M_impro04 ...\n",
      "Saved /workspace/dataset/S1/features/Ses01M_impro04.pt (52 utterances)\n",
      " Processing Ses01M_impro01 ...\n",
      "Saved /workspace/dataset/S1/features/Ses01M_impro01.pt (65 utterances)\n",
      " Processing Ses01M_impro06 ...\n",
      "Saved /workspace/dataset/S1/features/Ses01M_impro06.pt (60 utterances)\n",
      " Processing Ses01M_impro07 ...\n",
      "Saved /workspace/dataset/S1/features/Ses01M_impro07.pt (69 utterances)\n",
      " Processing Ses01M_script01_1 ...\n",
      "Saved /workspace/dataset/S1/features/Ses01M_script01_1.pt (87 utterances)\n",
      " Processing Ses01M_script01_2 ...\n",
      "Saved /workspace/dataset/S1/features/Ses01M_script01_2.pt (33 utterances)\n",
      " Processing Ses01M_script01_3 ...\n",
      "Saved /workspace/dataset/S1/features/Ses01M_script01_3.pt (75 utterances)\n",
      " Processing Ses01M_script02_2 ...\n",
      "Saved /workspace/dataset/S1/features/Ses01M_script02_2.pt (103 utterances)\n",
      " Processing Ses01M_script02_1 ...\n",
      "Saved /workspace/dataset/S1/features/Ses01M_script02_1.pt (72 utterances)\n",
      " Processing Ses01M_script03_1 ...\n",
      "Saved /workspace/dataset/S1/features/Ses01M_script03_1.pt (85 utterances)\n",
      " Processing Ses01M_script03_2 ...\n",
      "Saved /workspace/dataset/S1/features/Ses01M_script03_2.pt (86 utterances)\n",
      "All videos processed for this session.\n",
      "\n",
      "=== Processing S2 ===\n",
      " Processing Ses02F_impro01 ...\n",
      "Saved /workspace/dataset/S2/features/Ses02F_impro01.pt (49 utterances)\n",
      " Processing Ses02F_impro02 ...\n",
      "Saved /workspace/dataset/S2/features/Ses02F_impro02.pt (27 utterances)\n",
      " Processing Ses02F_impro03 ...\n",
      "Saved /workspace/dataset/S2/features/Ses02F_impro03.pt (58 utterances)\n",
      " Processing Ses02F_impro04 ...\n",
      "Saved /workspace/dataset/S2/features/Ses02F_impro04.pt (39 utterances)\n",
      " Processing Ses02F_impro05 ...\n",
      "Saved /workspace/dataset/S2/features/Ses02F_impro05.pt (45 utterances)\n",
      " Processing Ses02F_impro06 ...\n",
      "Saved /workspace/dataset/S2/features/Ses02F_impro06.pt (35 utterances)\n",
      " Processing Ses02F_impro07 ...\n",
      "Saved /workspace/dataset/S2/features/Ses02F_impro07.pt (74 utterances)\n",
      " Processing Ses02F_impro08 ...\n",
      "Saved /workspace/dataset/S2/features/Ses02F_impro08.pt (48 utterances)\n",
      " Processing Ses02F_script01_1 ...\n",
      "Saved /workspace/dataset/S2/features/Ses02F_script01_1.pt (90 utterances)\n",
      " Processing Ses02F_script01_2 ...\n",
      "Saved /workspace/dataset/S2/features/Ses02F_script01_2.pt (38 utterances)\n",
      " Processing Ses02F_script01_3 ...\n",
      "Saved /workspace/dataset/S2/features/Ses02F_script01_3.pt (67 utterances)\n",
      " Processing Ses02F_script02_1 ...\n",
      "Saved /workspace/dataset/S2/features/Ses02F_script02_1.pt (68 utterances)\n",
      " Processing Ses02F_script02_2 ...\n",
      "Saved /workspace/dataset/S2/features/Ses02F_script02_2.pt (93 utterances)\n",
      " Processing Ses02F_script03_1 ...\n",
      "Saved /workspace/dataset/S2/features/Ses02F_script03_1.pt (66 utterances)\n",
      " Processing Ses02F_script03_2 ...\n",
      "Saved /workspace/dataset/S2/features/Ses02F_script03_2.pt (92 utterances)\n",
      " Processing Ses02M_impro01 ...\n",
      "Saved /workspace/dataset/S2/features/Ses02M_impro01.pt (48 utterances)\n",
      " Processing Ses02M_impro02 ...\n",
      "Saved /workspace/dataset/S2/features/Ses02M_impro02.pt (34 utterances)\n",
      " Processing Ses02M_impro03 ...\n",
      "Saved /workspace/dataset/S2/features/Ses02M_impro03.pt (61 utterances)\n",
      " Processing Ses02M_impro04 ...\n",
      "Saved /workspace/dataset/S2/features/Ses02M_impro04.pt (41 utterances)\n",
      " Processing Ses02M_impro05 ...\n",
      "Saved /workspace/dataset/S2/features/Ses02M_impro05.pt (60 utterances)\n",
      " Processing Ses02M_impro06 ...\n",
      "Saved /workspace/dataset/S2/features/Ses02M_impro06.pt (43 utterances)\n",
      " Processing Ses02M_impro07 ...\n",
      "Saved /workspace/dataset/S2/features/Ses02M_impro07.pt (63 utterances)\n",
      " Processing Ses02M_impro08 ...\n",
      "Saved /workspace/dataset/S2/features/Ses02M_impro08.pt (63 utterances)\n",
      " Processing Ses02M_script01_1 ...\n",
      "Saved /workspace/dataset/S2/features/Ses02M_script01_1.pt (88 utterances)\n",
      " Processing Ses02M_script01_2 ...\n",
      "Saved /workspace/dataset/S2/features/Ses02M_script01_2.pt (37 utterances)\n",
      " Processing Ses02M_script01_3 ...\n",
      "Saved /workspace/dataset/S2/features/Ses02M_script01_3.pt (67 utterances)\n",
      " Processing Ses02M_script02_1 ...\n",
      "Saved /workspace/dataset/S2/features/Ses02M_script02_1.pt (68 utterances)\n",
      " Processing Ses02M_script02_2 ...\n",
      "Saved /workspace/dataset/S2/features/Ses02M_script02_2.pt (94 utterances)\n",
      " Processing Ses02M_script03_1 ...\n",
      "Saved /workspace/dataset/S2/features/Ses02M_script03_1.pt (64 utterances)\n",
      " Processing Ses02M_script03_2 ...\n",
      "Saved /workspace/dataset/S2/features/Ses02M_script03_2.pt (91 utterances)\n",
      "All videos processed for this session.\n",
      "\n",
      "=== Processing S3 ===\n",
      " Processing Ses03F_impro01 ...\n",
      "Saved /workspace/dataset/S3/features/Ses03F_impro01.pt (24 utterances)\n",
      " Processing Ses03F_impro02 ...\n",
      "Saved /workspace/dataset/S3/features/Ses03F_impro02.pt (76 utterances)\n",
      " Processing Ses03F_impro03 ...\n",
      "Saved /workspace/dataset/S3/features/Ses03F_impro03.pt (47 utterances)\n",
      " Processing Ses03F_impro04 ...\n",
      "Saved /workspace/dataset/S3/features/Ses03F_impro04.pt (48 utterances)\n",
      " Processing Ses03F_impro05 ...\n",
      "Saved /workspace/dataset/S3/features/Ses03F_impro05.pt (34 utterances)\n",
      " Processing Ses03F_impro06 ...\n",
      "Saved /workspace/dataset/S3/features/Ses03F_impro06.pt (57 utterances)\n",
      " Processing Ses03F_impro07 ...\n",
      "Saved /workspace/dataset/S3/features/Ses03F_impro07.pt (76 utterances)\n",
      " Processing Ses03F_impro08 ...\n",
      "Saved /workspace/dataset/S3/features/Ses03F_impro08.pt (59 utterances)\n",
      " Processing Ses03F_script01_1 ...\n",
      "Saved /workspace/dataset/S3/features/Ses03F_script01_1.pt (108 utterances)\n",
      " Processing Ses03F_script01_2 ...\n",
      "Saved /workspace/dataset/S3/features/Ses03F_script01_2.pt (41 utterances)\n",
      " Processing Ses03F_script01_3 ...\n",
      "Saved /workspace/dataset/S3/features/Ses03F_script01_3.pt (83 utterances)\n",
      " Processing Ses03F_script02_1 ...\n",
      "Saved /workspace/dataset/S3/features/Ses03F_script02_1.pt (67 utterances)\n",
      " Processing Ses03F_script02_2 ...\n",
      "Saved /workspace/dataset/S3/features/Ses03F_script02_2.pt (99 utterances)\n",
      " Processing Ses03F_script03_1 ...\n",
      "Saved /workspace/dataset/S3/features/Ses03F_script03_1.pt (61 utterances)\n",
      " Processing Ses03F_script03_2 ...\n",
      "Saved /workspace/dataset/S3/features/Ses03F_script03_2.pt (89 utterances)\n",
      " Processing Ses03M_impro01 ...\n",
      "Saved /workspace/dataset/S3/features/Ses03M_impro01.pt (56 utterances)\n",
      " Processing Ses03M_impro02 ...\n",
      "Saved /workspace/dataset/S3/features/Ses03M_impro02.pt (76 utterances)\n",
      " Processing Ses03M_impro03 ...\n",
      "Saved /workspace/dataset/S3/features/Ses03M_impro03.pt (84 utterances)\n",
      " Processing Ses03M_impro04 ...\n",
      "Saved /workspace/dataset/S3/features/Ses03M_impro04.pt (78 utterances)\n",
      " Processing Ses03M_impro05a ...\n",
      "Saved /workspace/dataset/S3/features/Ses03M_impro05a.pt (60 utterances)\n",
      " Processing Ses03M_impro05b ...\n",
      "Saved /workspace/dataset/S3/features/Ses03M_impro05b.pt (69 utterances)\n",
      " Processing Ses03M_impro06 ...\n",
      "Saved /workspace/dataset/S3/features/Ses03M_impro06.pt (55 utterances)\n",
      " Processing Ses03M_impro07 ...\n",
      "Saved /workspace/dataset/S3/features/Ses03M_impro07.pt (52 utterances)\n",
      " Processing Ses03M_impro08a ...\n",
      "Saved /workspace/dataset/S3/features/Ses03M_impro08a.pt (66 utterances)\n",
      " Processing Ses03M_impro08b ...\n",
      "Saved /workspace/dataset/S3/features/Ses03M_impro08b.pt (49 utterances)\n",
      " Processing Ses03M_script01_1 ...\n",
      "Saved /workspace/dataset/S3/features/Ses03M_script01_1.pt (95 utterances)\n",
      " Processing Ses03M_script01_2 ...\n",
      "Saved /workspace/dataset/S3/features/Ses03M_script01_2.pt (38 utterances)\n",
      " Processing Ses03M_script01_3 ...\n",
      "Saved /workspace/dataset/S3/features/Ses03M_script01_3.pt (82 utterances)\n",
      " Processing Ses03M_script02_1 ...\n",
      "Saved /workspace/dataset/S3/features/Ses03M_script02_1.pt (68 utterances)\n",
      " Processing Ses03M_script02_2 ...\n",
      "Saved /workspace/dataset/S3/features/Ses03M_script02_2.pt (97 utterances)\n",
      " Processing Ses03M_script03_1 ...\n",
      "Saved /workspace/dataset/S3/features/Ses03M_script03_1.pt (71 utterances)\n",
      " Processing Ses03M_script03_2 ...\n",
      "Saved /workspace/dataset/S3/features/Ses03M_script03_2.pt (93 utterances)\n",
      "All videos processed for this session.\n",
      "\n",
      "=== Processing S4 ===\n",
      " Processing Ses04F_impro01 ...\n",
      "Saved /workspace/dataset/S4/features/Ses04F_impro01.pt (49 utterances)\n",
      " Processing Ses04F_impro02 ...\n",
      "Saved /workspace/dataset/S4/features/Ses04F_impro02.pt (52 utterances)\n",
      " Processing Ses04F_impro03 ...\n",
      "Saved /workspace/dataset/S4/features/Ses04F_impro03.pt (104 utterances)\n",
      " Processing Ses04F_impro04 ...\n",
      "Saved /workspace/dataset/S4/features/Ses04F_impro04.pt (69 utterances)\n",
      " Processing Ses04F_impro05 ...\n",
      "Saved /workspace/dataset/S4/features/Ses04F_impro05.pt (45 utterances)\n",
      " Processing Ses04F_impro06 ...\n",
      "Saved /workspace/dataset/S4/features/Ses04F_impro06.pt (26 utterances)\n",
      " Processing Ses04F_impro07 ...\n",
      "Saved /workspace/dataset/S4/features/Ses04F_impro07.pt (167 utterances)\n",
      " Processing Ses04F_impro08 ...\n",
      "Saved /workspace/dataset/S4/features/Ses04F_impro08.pt (58 utterances)\n",
      " Processing Ses04F_script01_1 ...\n",
      "Saved /workspace/dataset/S4/features/Ses04F_script01_1.pt (91 utterances)\n",
      " Processing Ses04F_script01_3 ...\n",
      "Saved /workspace/dataset/S4/features/Ses04F_script01_3.pt (79 utterances)\n",
      " Processing Ses04F_script01_2 ...\n",
      "Saved /workspace/dataset/S4/features/Ses04F_script01_2.pt (41 utterances)\n",
      " Processing Ses04F_script02_2 ...\n",
      "Saved /workspace/dataset/S4/features/Ses04F_script02_2.pt (99 utterances)\n",
      " Processing Ses04F_script03_1 ...\n",
      "Saved /workspace/dataset/S4/features/Ses04F_script03_1.pt (68 utterances)\n",
      " Processing Ses04F_script03_2 ...\n",
      "Saved /workspace/dataset/S4/features/Ses04F_script03_2.pt (91 utterances)\n",
      " Processing Ses04M_impro01 ...\n",
      "Saved /workspace/dataset/S4/features/Ses04M_impro01.pt (47 utterances)\n",
      " Processing Ses04M_impro02 ...\n",
      "Saved /workspace/dataset/S4/features/Ses04M_impro02.pt (68 utterances)\n",
      " Processing Ses04M_impro03 ...\n",
      "Saved /workspace/dataset/S4/features/Ses04M_impro03.pt (58 utterances)\n",
      " Processing Ses04M_impro04 ...\n",
      "Saved /workspace/dataset/S4/features/Ses04M_impro04.pt (43 utterances)\n",
      " Processing Ses04M_impro05 ...\n",
      "Saved /workspace/dataset/S4/features/Ses04M_impro05.pt (89 utterances)\n",
      " Processing Ses04M_impro06 ...\n",
      "Saved /workspace/dataset/S4/features/Ses04M_impro06.pt (41 utterances)\n",
      " Processing Ses04F_script02_1 ...\n",
      "Saved /workspace/dataset/S4/features/Ses04F_script02_1.pt (66 utterances)\n",
      " Processing Ses04M_script01_1 ...\n",
      "Saved /workspace/dataset/S4/features/Ses04M_script01_1.pt (89 utterances)\n",
      " Processing Ses04M_script01_2 ...\n",
      "Saved /workspace/dataset/S4/features/Ses04M_script01_2.pt (41 utterances)\n",
      " Processing Ses04M_script01_3 ...\n",
      "Saved /workspace/dataset/S4/features/Ses04M_script01_3.pt (66 utterances)\n",
      " Processing Ses04M_impro08 ...\n",
      "Saved /workspace/dataset/S4/features/Ses04M_impro08.pt (60 utterances)\n",
      " Processing Ses04M_script02_2 ...\n",
      "Saved /workspace/dataset/S4/features/Ses04M_script02_2.pt (81 utterances)\n",
      " Processing Ses04M_script02_1 ...\n",
      "Saved /workspace/dataset/S4/features/Ses04M_script02_1.pt (56 utterances)\n",
      " Processing Ses04M_script03_1 ...\n",
      "Saved /workspace/dataset/S4/features/Ses04M_script03_1.pt (65 utterances)\n",
      " Processing Ses04M_script03_2 ...\n",
      "Saved /workspace/dataset/S4/features/Ses04M_script03_2.pt (110 utterances)\n",
      " Processing Ses04M_impro07 ...\n",
      "Saved /workspace/dataset/S4/features/Ses04M_impro07.pt (84 utterances)\n",
      "All videos processed for this session.\n",
      "\n",
      "=== Processing S5 ===\n",
      " Processing Ses05F_impro01 ...\n",
      "Saved /workspace/dataset/S5/features/Ses05F_impro01.pt (50 utterances)\n",
      " Processing Ses05F_impro02 ...\n",
      "Saved /workspace/dataset/S5/features/Ses05F_impro02.pt (83 utterances)\n",
      " Processing Ses05F_impro03 ...\n",
      "Saved /workspace/dataset/S5/features/Ses05F_impro03.pt (135 utterances)\n",
      " Processing Ses05F_impro04 ...\n",
      "Saved /workspace/dataset/S5/features/Ses05F_impro04.pt (92 utterances)\n",
      " Processing Ses05F_impro05 ...\n",
      "Saved /workspace/dataset/S5/features/Ses05F_impro05.pt (100 utterances)\n",
      " Processing Ses05F_impro06 ...\n",
      "Saved /workspace/dataset/S5/features/Ses05F_impro06.pt (56 utterances)\n",
      " Processing Ses05F_impro07 ...\n",
      "Saved /workspace/dataset/S5/features/Ses05F_impro07.pt (77 utterances)\n",
      " Processing Ses05F_impro08 ...\n",
      "Saved /workspace/dataset/S5/features/Ses05F_impro08.pt (68 utterances)\n",
      " Processing Ses05F_script01_1 ...\n",
      "Saved /workspace/dataset/S5/features/Ses05F_script01_1.pt (83 utterances)\n",
      " Processing Ses05F_script01_2 ...\n",
      "Saved /workspace/dataset/S5/features/Ses05F_script01_2.pt (36 utterances)\n",
      " Processing Ses05F_script01_3 ...\n",
      "Saved /workspace/dataset/S5/features/Ses05F_script01_3.pt (68 utterances)\n",
      " Processing Ses05F_script02_1 ...\n",
      "Saved /workspace/dataset/S5/features/Ses05F_script02_1.pt (56 utterances)\n",
      " Processing Ses05F_script02_2 ...\n",
      "Saved /workspace/dataset/S5/features/Ses05F_script02_2.pt (82 utterances)\n",
      " Processing Ses05F_script03_1 ...\n",
      "Saved /workspace/dataset/S5/features/Ses05F_script03_1.pt (68 utterances)\n",
      " Processing Ses05F_script03_2 ...\n",
      "Saved /workspace/dataset/S5/features/Ses05F_script03_2.pt (86 utterances)\n",
      " Processing Ses05M_impro01 ...\n",
      "Saved /workspace/dataset/S5/features/Ses05M_impro01.pt (47 utterances)\n",
      " Processing Ses05M_impro02 ...\n",
      "Saved /workspace/dataset/S5/features/Ses05M_impro02.pt (59 utterances)\n",
      " Processing Ses05M_impro03 ...\n",
      "Saved /workspace/dataset/S5/features/Ses05M_impro03.pt (68 utterances)\n",
      " Processing Ses05M_impro04 ...\n",
      "Saved /workspace/dataset/S5/features/Ses05M_impro04.pt (81 utterances)\n",
      " Processing Ses05M_impro05 ...\n",
      "Saved /workspace/dataset/S5/features/Ses05M_impro05.pt (45 utterances)\n",
      " Processing Ses05M_impro06 ...\n",
      "Saved /workspace/dataset/S5/features/Ses05M_impro06.pt (34 utterances)\n",
      " Processing Ses05M_impro07 ...\n",
      "Saved /workspace/dataset/S5/features/Ses05M_impro07.pt (98 utterances)\n",
      " Processing Ses05M_impro08 ...\n",
      "Saved /workspace/dataset/S5/features/Ses05M_impro08.pt (60 utterances)\n",
      " Processing Ses05M_script01_1 ...\n",
      "Saved /workspace/dataset/S5/features/Ses05M_script01_1.pt (80 utterances)\n",
      " Processing Ses05M_script01_1b ...\n",
      "Saved /workspace/dataset/S5/features/Ses05M_script01_1b.pt (81 utterances)\n",
      " Processing Ses05M_script01_2 ...\n",
      "Saved /workspace/dataset/S5/features/Ses05M_script01_2.pt (35 utterances)\n",
      " Processing Ses05M_script01_3 ...\n",
      "Saved /workspace/dataset/S5/features/Ses05M_script01_3.pt (71 utterances)\n",
      " Processing Ses05M_script02_1 ...\n",
      "Saved /workspace/dataset/S5/features/Ses05M_script02_1.pt (55 utterances)\n",
      " Processing Ses05M_script02_2 ...\n",
      "Saved /workspace/dataset/S5/features/Ses05M_script02_2.pt (81 utterances)\n",
      " Processing Ses05M_script03_1 ...\n",
      "Saved /workspace/dataset/S5/features/Ses05M_script03_1.pt (67 utterances)\n",
      " Processing Ses05M_script03_2 ...\n",
      "Saved /workspace/dataset/S5/features/Ses05M_script03_2.pt (91 utterances)\n",
      "All videos processed for this session.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "from torchvision.transforms import Compose, Lambda\n",
    "from pytorchvideo.transforms import UniformTemporalSubsample\n",
    "from imagebind import data\n",
    "from imagebind.models import imagebind_model\n",
    "from imagebind.models.imagebind_model import ModalityType\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# ===== 初始化模型 =====\n",
    "device = \"cuda:0\"\n",
    "model = imagebind_model.imagebind_huge(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False \n",
    "model.eval().to(device)\n",
    "\n",
    "\n",
    "# ====== 视频预处理函数 ======\n",
    "def load_and_transform_vision_data_from_tensor(video_tensor, device):\n",
    "    mean = torch.tensor([0.48145466, 0.4578275, 0.40821073], device=device)[:, None, None, None]\n",
    "    std = torch.tensor([0.26862954, 0.26130258, 0.27577711], device=device)[:, None, None, None]\n",
    "\n",
    "    transform = T.Compose([T.Resize((224, 224))])\n",
    "    frames = []\n",
    "    for t in range(video_tensor.shape[1]):\n",
    "        frame = video_tensor[:, t, :, :].cpu()\n",
    "        frame = T.ToPILImage()(frame)\n",
    "        frame = transform(frame)\n",
    "        frame = T.ToTensor()(frame)\n",
    "        frames.append(frame)\n",
    "\n",
    "    x = torch.stack(frames, dim=0).to(device)\n",
    "    x = (x - mean.permute(1, 0, 2, 3)) / std.permute(1, 0, 2, 3)\n",
    "    return x\n",
    "\n",
    "\n",
    "def load_video_gpu(video_path, fps=8, max_duration=15.0):\n",
    "    video = EncodedVideo.from_path(video_path, decoder=\"decord\", decode_audio=False)\n",
    "    duration = getattr(video, \"duration\", None)\n",
    "    if duration is None or duration <= 0:\n",
    "        print(f\"[WARN] Invalid duration for {video_path}, skipping.\")\n",
    "        return None\n",
    "\n",
    "    duration = min(duration, max_duration)\n",
    "    clip = video.get_clip(start_sec=0, end_sec=duration)\n",
    "    if clip is None or clip.get(\"video\") is None:\n",
    "        print(f\"[WARN] Failed to read video from {video_path}\")\n",
    "        return None\n",
    "\n",
    "    video_data = clip[\"video\"]\n",
    "    if video_data is None:\n",
    "        print(f\"[WARN] Empty video data for {video_path}\")\n",
    "        return None\n",
    "\n",
    "    target_frames = int(math.ceil(fps * duration))\n",
    "    T = video_data.shape[1]\n",
    "    if T > target_frames:\n",
    "        indices = torch.linspace(0, T - 1, target_frames).long()\n",
    "        video_data = video_data[:, indices, :, :]\n",
    "    elif T < target_frames and T > 0:\n",
    "        repeat = (target_frames // T) + 1\n",
    "        video_data = video_data.repeat(1, repeat, 1, 1)[:, :target_frames]\n",
    "    return video_data\n",
    "\n",
    "\n",
    "# ====== 提取 Embedding 函数 ======\n",
    "def extract_embeddings(meta_file, save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    with open(meta_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        items = json.load(f)\n",
    "\n",
    "    grouped = {}\n",
    "    for item in items:\n",
    "        vid = item[\"video_path\"].split(\"/\")[-2]\n",
    "        grouped.setdefault(vid, []).append(item)\n",
    "\n",
    "    for vid, utterances in grouped.items():\n",
    "        print(f\" Processing {vid} ...\")\n",
    "        all_embeddings = {}\n",
    "\n",
    "        for item in utterances:\n",
    "            utt_id = item[\"utt_id\"]\n",
    "            text = item[\"text\"]\n",
    "            audio_path = item[\"audio_path\"]\n",
    "            video_path = item[\"video_path\"]\n",
    "\n",
    "            start_time = item.get(\"start_time\", None)\n",
    "            end_time = item.get(\"end_time\", None)\n",
    "\n",
    "            video_tensor = load_video_gpu(video_path)\n",
    "            vision_input = load_and_transform_vision_data_from_tensor(video_tensor, device)\n",
    "            text_input = data.load_and_transform_text([text], device)\n",
    "            audio_input = data.load_and_transform_audio_data([audio_path], device)\n",
    "\n",
    "            inputs = {\n",
    "                ModalityType.TEXT: text_input,\n",
    "                ModalityType.AUDIO: audio_input,\n",
    "                ModalityType.VISION: vision_input,\n",
    "            }\n",
    "\n",
    "            with torch.no_grad():\n",
    "                emb = model(inputs)\n",
    "\n",
    "            all_embeddings[utt_id] = {\n",
    "                \"text_emb\": emb[ModalityType.TEXT].cpu(),\n",
    "                \"audio_emb\": emb[ModalityType.AUDIO].cpu(),\n",
    "                \"vision_emb\": emb[ModalityType.VISION].mean(dim=0, keepdim=True).cpu(),\n",
    "                \"start_time\": start_time,\n",
    "                \"end_time\": end_time,\n",
    "                \"text\": text,\n",
    "            }\n",
    "\n",
    "        save_path = os.path.join(save_dir, f\"{vid}.pt\")\n",
    "        torch.save(all_embeddings, save_path)\n",
    "        print(f\"Saved {save_path} ({len(all_embeddings)} utterances)\")\n",
    "\n",
    "    print(\"All videos processed for this session.\")\n",
    "\n",
    "\n",
    "# ====== 批量处理 S1–S5 ======\n",
    "if __name__ == \"__main__\":\n",
    "    base_dir = \"/workspace/dataset\"\n",
    "    sessions = [\"S1\", \"S2\", \"S3\", \"S4\", \"S5\"]\n",
    "\n",
    "    for s in sessions:\n",
    "        meta_path = os.path.join(base_dir, s, \"meta\", \"train_meta.json\")\n",
    "        save_dir = os.path.join(base_dir, s, \"features\")\n",
    "\n",
    "        if not os.path.exists(meta_path):\n",
    "            print(f\"[WARN] {meta_path} not found, skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n=== Processing {s} ===\")\n",
    "        extract_embeddings(meta_path, save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab5bd4a5-8eb8-47d6-a51d-99b3c070e0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging Ses01F_impro01.pt\n",
      "Saved: /workspace/dataset/S1/features/Ses01F_impro01_with_emo.pt (Matched 30, Missed 0)\n",
      "\n",
      "Merging Ses01F_impro02.pt\n",
      "Saved: /workspace/dataset/S1/features/Ses01F_impro02_with_emo.pt (Matched 38, Missed 0)\n",
      "\n",
      "Merging Ses01F_impro03.pt\n",
      "Saved: /workspace/dataset/S1/features/Ses01F_impro03_with_emo.pt (Matched 52, Missed 0)\n",
      "\n",
      "Merging Ses01F_impro04.pt\n",
      "Saved: /workspace/dataset/S1/features/Ses01F_impro04_with_emo.pt (Matched 71, Missed 0)\n",
      "\n",
      "Merging Ses01F_impro05.pt\n",
      "Saved: /workspace/dataset/S1/features/Ses01F_impro05_with_emo.pt (Matched 67, Missed 0)\n",
      "\n",
      "Merging Ses01F_impro06.pt\n",
      "Saved: /workspace/dataset/S1/features/Ses01F_impro06_with_emo.pt (Matched 47, Missed 0)\n",
      "\n",
      "Merging Ses01F_impro07.pt\n",
      "Saved: /workspace/dataset/S1/features/Ses01F_impro07_with_emo.pt (Matched 37, Missed 0)\n",
      "\n",
      "Merging Ses01F_script01_1.pt\n",
      "Saved: /workspace/dataset/S1/features/Ses01F_script01_1_with_emo.pt (Matched 89, Missed 0)\n",
      "\n",
      "Merging Ses01F_script01_2.pt\n",
      "Saved: /workspace/dataset/S1/features/Ses01F_script01_2_with_emo.pt (Matched 33, Missed 0)\n",
      "\n",
      "Merging Ses01F_script01_3.pt\n",
      "Saved: /workspace/dataset/S1/features/Ses01F_script01_3_with_emo.pt (Matched 75, Missed 0)\n",
      "\n",
      "Merging Ses01F_script02_2.pt\n",
      "Saved: /workspace/dataset/S1/features/Ses01F_script02_2_with_emo.pt (Matched 98, Missed 0)\n",
      "\n",
      "Merging Ses01F_script02_1.pt\n",
      "Saved: /workspace/dataset/S1/features/Ses01F_script02_1_with_emo.pt (Matched 71, Missed 0)\n",
      "\n",
      "Merging Ses01F_script03_1.pt\n",
      "Saved: /workspace/dataset/S1/features/Ses01F_script03_1_with_emo.pt (Matched 71, Missed 0)\n",
      "\n",
      "Merging Ses01F_script03_2.pt\n",
      "Saved: /workspace/dataset/S1/features/Ses01F_script03_2_with_emo.pt (Matched 82, Missed 0)\n",
      "\n",
      "Merging Ses01M_impro05.pt\n",
      "Saved: /workspace/dataset/S1/features/Ses01M_impro05_with_emo.pt (Matched 70, Missed 0)\n",
      "\n",
      "Merging Ses01M_impro02.pt\n",
      "Saved: /workspace/dataset/S1/features/Ses01M_impro02_with_emo.pt (Matched 48, Missed 0)\n",
      "\n",
      "Merging Ses01M_impro03.pt\n",
      "Saved: /workspace/dataset/S1/features/Ses01M_impro03_with_emo.pt (Matched 53, Missed 0)\n",
      "\n",
      "Merging Ses01M_impro04.pt\n",
      "Saved: /workspace/dataset/S1/features/Ses01M_impro04_with_emo.pt (Matched 52, Missed 0)\n",
      "\n",
      "Merging Ses01M_impro01.pt\n",
      "Saved: /workspace/dataset/S1/features/Ses01M_impro01_with_emo.pt (Matched 65, Missed 0)\n",
      "\n",
      "Merging Ses01M_impro06.pt\n",
      "Saved: /workspace/dataset/S1/features/Ses01M_impro06_with_emo.pt (Matched 60, Missed 0)\n",
      "\n",
      "Merging Ses01M_impro07.pt\n",
      "Saved: /workspace/dataset/S1/features/Ses01M_impro07_with_emo.pt (Matched 69, Missed 0)\n",
      "\n",
      "Merging Ses01M_script01_1.pt\n",
      "Saved: /workspace/dataset/S1/features/Ses01M_script01_1_with_emo.pt (Matched 87, Missed 0)\n",
      "\n",
      "Merging Ses01M_script01_2.pt\n",
      "Saved: /workspace/dataset/S1/features/Ses01M_script01_2_with_emo.pt (Matched 33, Missed 0)\n",
      "\n",
      "Merging Ses01M_script01_3.pt\n",
      "Saved: /workspace/dataset/S1/features/Ses01M_script01_3_with_emo.pt (Matched 75, Missed 0)\n",
      "\n",
      "Merging Ses01M_script02_2.pt\n",
      "Saved: /workspace/dataset/S1/features/Ses01M_script02_2_with_emo.pt (Matched 103, Missed 0)\n",
      "\n",
      "Merging Ses01M_script02_1.pt\n",
      "Saved: /workspace/dataset/S1/features/Ses01M_script02_1_with_emo.pt (Matched 72, Missed 0)\n",
      "\n",
      "Merging Ses01M_script03_1.pt\n",
      "Saved: /workspace/dataset/S1/features/Ses01M_script03_1_with_emo.pt (Matched 85, Missed 0)\n",
      "\n",
      "Merging Ses01M_script03_2.pt\n",
      "Saved: /workspace/dataset/S1/features/Ses01M_script03_2_with_emo.pt (Matched 86, Missed 0)\n",
      "\n",
      "Merging Ses02F_impro01.pt\n",
      "Saved: /workspace/dataset/S2/features/Ses02F_impro01_with_emo.pt (Matched 49, Missed 0)\n",
      "\n",
      "Merging Ses02F_impro02.pt\n",
      "Saved: /workspace/dataset/S2/features/Ses02F_impro02_with_emo.pt (Matched 27, Missed 0)\n",
      "\n",
      "Merging Ses02F_impro03.pt\n",
      "Saved: /workspace/dataset/S2/features/Ses02F_impro03_with_emo.pt (Matched 58, Missed 0)\n",
      "\n",
      "Merging Ses02F_impro04.pt\n",
      "Saved: /workspace/dataset/S2/features/Ses02F_impro04_with_emo.pt (Matched 39, Missed 0)\n",
      "\n",
      "Merging Ses02F_impro05.pt\n",
      "Saved: /workspace/dataset/S2/features/Ses02F_impro05_with_emo.pt (Matched 45, Missed 0)\n",
      "\n",
      "Merging Ses02F_impro06.pt\n",
      "Saved: /workspace/dataset/S2/features/Ses02F_impro06_with_emo.pt (Matched 35, Missed 0)\n",
      "\n",
      "Merging Ses02F_impro07.pt\n",
      "Saved: /workspace/dataset/S2/features/Ses02F_impro07_with_emo.pt (Matched 74, Missed 0)\n",
      "\n",
      "Merging Ses02F_impro08.pt\n",
      "Saved: /workspace/dataset/S2/features/Ses02F_impro08_with_emo.pt (Matched 48, Missed 0)\n",
      "\n",
      "Merging Ses02F_script01_1.pt\n",
      "Saved: /workspace/dataset/S2/features/Ses02F_script01_1_with_emo.pt (Matched 90, Missed 0)\n",
      "\n",
      "Merging Ses02F_script01_2.pt\n",
      "Saved: /workspace/dataset/S2/features/Ses02F_script01_2_with_emo.pt (Matched 38, Missed 0)\n",
      "\n",
      "Merging Ses02F_script01_3.pt\n",
      "Saved: /workspace/dataset/S2/features/Ses02F_script01_3_with_emo.pt (Matched 67, Missed 0)\n",
      "\n",
      "Merging Ses02F_script02_1.pt\n",
      "Saved: /workspace/dataset/S2/features/Ses02F_script02_1_with_emo.pt (Matched 68, Missed 0)\n",
      "\n",
      "Merging Ses02F_script02_2.pt\n",
      "Saved: /workspace/dataset/S2/features/Ses02F_script02_2_with_emo.pt (Matched 93, Missed 0)\n",
      "\n",
      "Merging Ses02F_script03_1.pt\n",
      "Saved: /workspace/dataset/S2/features/Ses02F_script03_1_with_emo.pt (Matched 66, Missed 0)\n",
      "\n",
      "Merging Ses02F_script03_2.pt\n",
      "Saved: /workspace/dataset/S2/features/Ses02F_script03_2_with_emo.pt (Matched 92, Missed 0)\n",
      "\n",
      "Merging Ses02M_impro01.pt\n",
      "Saved: /workspace/dataset/S2/features/Ses02M_impro01_with_emo.pt (Matched 48, Missed 0)\n",
      "\n",
      "Merging Ses02M_impro02.pt\n",
      "Saved: /workspace/dataset/S2/features/Ses02M_impro02_with_emo.pt (Matched 34, Missed 0)\n",
      "\n",
      "Merging Ses02M_impro03.pt\n",
      "Saved: /workspace/dataset/S2/features/Ses02M_impro03_with_emo.pt (Matched 61, Missed 0)\n",
      "\n",
      "Merging Ses02M_impro04.pt\n",
      "Saved: /workspace/dataset/S2/features/Ses02M_impro04_with_emo.pt (Matched 41, Missed 0)\n",
      "\n",
      "Merging Ses02M_impro05.pt\n",
      "Saved: /workspace/dataset/S2/features/Ses02M_impro05_with_emo.pt (Matched 60, Missed 0)\n",
      "\n",
      "Merging Ses02M_impro06.pt\n",
      "Saved: /workspace/dataset/S2/features/Ses02M_impro06_with_emo.pt (Matched 43, Missed 0)\n",
      "\n",
      "Merging Ses02M_impro07.pt\n",
      "Saved: /workspace/dataset/S2/features/Ses02M_impro07_with_emo.pt (Matched 63, Missed 0)\n",
      "\n",
      "Merging Ses02M_impro08.pt\n",
      "Saved: /workspace/dataset/S2/features/Ses02M_impro08_with_emo.pt (Matched 63, Missed 0)\n",
      "\n",
      "Merging Ses02M_script01_1.pt\n",
      "Saved: /workspace/dataset/S2/features/Ses02M_script01_1_with_emo.pt (Matched 88, Missed 0)\n",
      "\n",
      "Merging Ses02M_script01_2.pt\n",
      "Saved: /workspace/dataset/S2/features/Ses02M_script01_2_with_emo.pt (Matched 37, Missed 0)\n",
      "\n",
      "Merging Ses02M_script01_3.pt\n",
      "Saved: /workspace/dataset/S2/features/Ses02M_script01_3_with_emo.pt (Matched 67, Missed 0)\n",
      "\n",
      "Merging Ses02M_script02_1.pt\n",
      "Saved: /workspace/dataset/S2/features/Ses02M_script02_1_with_emo.pt (Matched 68, Missed 0)\n",
      "\n",
      "Merging Ses02M_script02_2.pt\n",
      "Saved: /workspace/dataset/S2/features/Ses02M_script02_2_with_emo.pt (Matched 94, Missed 0)\n",
      "\n",
      "Merging Ses02M_script03_1.pt\n",
      "Saved: /workspace/dataset/S2/features/Ses02M_script03_1_with_emo.pt (Matched 64, Missed 0)\n",
      "\n",
      "Merging Ses02M_script03_2.pt\n",
      "Saved: /workspace/dataset/S2/features/Ses02M_script03_2_with_emo.pt (Matched 91, Missed 0)\n",
      "\n",
      "Merging Ses03F_impro01.pt\n",
      "Saved: /workspace/dataset/S3/features/Ses03F_impro01_with_emo.pt (Matched 24, Missed 0)\n",
      "\n",
      "Merging Ses03F_impro02.pt\n",
      "Saved: /workspace/dataset/S3/features/Ses03F_impro02_with_emo.pt (Matched 76, Missed 0)\n",
      "\n",
      "Merging Ses03F_impro03.pt\n",
      "Saved: /workspace/dataset/S3/features/Ses03F_impro03_with_emo.pt (Matched 47, Missed 0)\n",
      "\n",
      "Merging Ses03F_impro04.pt\n",
      "Saved: /workspace/dataset/S3/features/Ses03F_impro04_with_emo.pt (Matched 48, Missed 0)\n",
      "\n",
      "Merging Ses03F_impro05.pt\n",
      "No label for Ses03F_impro05_MXX0\n",
      "Saved: /workspace/dataset/S3/features/Ses03F_impro05_with_emo.pt (Matched 33, Missed 1)\n",
      "\n",
      "Merging Ses03F_impro06.pt\n",
      "No label for Ses03F_impro06_MXX0\n",
      "No label for Ses03F_impro06_FXX0\n",
      "No label for Ses03F_impro06_MXX1\n",
      "No label for Ses03F_impro06_FXX1\n",
      "Saved: /workspace/dataset/S3/features/Ses03F_impro06_with_emo.pt (Matched 53, Missed 4)\n",
      "\n",
      "Merging Ses03F_impro07.pt\n",
      "No label for Ses03F_impro07_MXX0\n",
      "No label for Ses03F_impro07_MXX1\n",
      "No label for Ses03F_impro07_MXX2\n",
      "No label for Ses03F_impro07_MXX3\n",
      "Saved: /workspace/dataset/S3/features/Ses03F_impro07_with_emo.pt (Matched 72, Missed 4)\n",
      "\n",
      "Merging Ses03F_impro08.pt\n",
      "No label for Ses03F_impro08_FXX0\n",
      "No label for Ses03F_impro08_MXX0\n",
      "Saved: /workspace/dataset/S3/features/Ses03F_impro08_with_emo.pt (Matched 57, Missed 2)\n",
      "\n",
      "Merging Ses03F_script01_1.pt\n",
      "Saved: /workspace/dataset/S3/features/Ses03F_script01_1_with_emo.pt (Matched 108, Missed 0)\n",
      "\n",
      "Merging Ses03F_script01_2.pt\n",
      "Saved: /workspace/dataset/S3/features/Ses03F_script01_2_with_emo.pt (Matched 41, Missed 0)\n",
      "\n",
      "Merging Ses03F_script01_3.pt\n",
      "Saved: /workspace/dataset/S3/features/Ses03F_script01_3_with_emo.pt (Matched 83, Missed 0)\n",
      "\n",
      "Merging Ses03F_script02_1.pt\n",
      "Saved: /workspace/dataset/S3/features/Ses03F_script02_1_with_emo.pt (Matched 67, Missed 0)\n",
      "\n",
      "Merging Ses03F_script02_2.pt\n",
      "Saved: /workspace/dataset/S3/features/Ses03F_script02_2_with_emo.pt (Matched 99, Missed 0)\n",
      "\n",
      "Merging Ses03F_script03_1.pt\n",
      "Saved: /workspace/dataset/S3/features/Ses03F_script03_1_with_emo.pt (Matched 61, Missed 0)\n",
      "\n",
      "Merging Ses03F_script03_2.pt\n",
      "Saved: /workspace/dataset/S3/features/Ses03F_script03_2_with_emo.pt (Matched 89, Missed 0)\n",
      "\n",
      "Merging Ses03M_impro01.pt\n",
      "Saved: /workspace/dataset/S3/features/Ses03M_impro01_with_emo.pt (Matched 56, Missed 0)\n",
      "\n",
      "Merging Ses03M_impro02.pt\n",
      "Saved: /workspace/dataset/S3/features/Ses03M_impro02_with_emo.pt (Matched 76, Missed 0)\n",
      "\n",
      "Merging Ses03M_impro03.pt\n",
      "Saved: /workspace/dataset/S3/features/Ses03M_impro03_with_emo.pt (Matched 84, Missed 0)\n",
      "\n",
      "Merging Ses03M_impro04.pt\n",
      "No label for Ses03M_impro04_FXX0\n",
      "No label for Ses03M_impro04_FXX1\n",
      "No label for Ses03M_impro04_MXX0\n",
      "Saved: /workspace/dataset/S3/features/Ses03M_impro04_with_emo.pt (Matched 75, Missed 3)\n",
      "\n",
      "Merging Ses03M_impro05a.pt\n",
      "Saved: /workspace/dataset/S3/features/Ses03M_impro05a_with_emo.pt (Matched 60, Missed 0)\n",
      "\n",
      "Merging Ses03M_impro05b.pt\n",
      "No label for Ses03M_impro05b_FXX0\n",
      "No label for Ses03M_impro05b_FXX1\n",
      "No label for Ses03M_impro05b_FXX2\n",
      "Saved: /workspace/dataset/S3/features/Ses03M_impro05b_with_emo.pt (Matched 66, Missed 3)\n",
      "\n",
      "Merging Ses03M_impro06.pt\n",
      "No label for Ses03M_impro06_MXX0\n",
      "Saved: /workspace/dataset/S3/features/Ses03M_impro06_with_emo.pt (Matched 54, Missed 1)\n",
      "\n",
      "Merging Ses03M_impro07.pt\n",
      "No label for Ses03M_impro07_MXX0\n",
      "Saved: /workspace/dataset/S3/features/Ses03M_impro07_with_emo.pt (Matched 51, Missed 1)\n",
      "\n",
      "Merging Ses03M_impro08a.pt\n",
      "No label for Ses03M_impro08a_MXX0\n",
      "No label for Ses03M_impro08a_MXX1\n",
      "Saved: /workspace/dataset/S3/features/Ses03M_impro08a_with_emo.pt (Matched 64, Missed 2)\n",
      "\n",
      "Merging Ses03M_impro08b.pt\n",
      "No label for Ses03M_impro08b_MXX0\n",
      "Saved: /workspace/dataset/S3/features/Ses03M_impro08b_with_emo.pt (Matched 48, Missed 1)\n",
      "\n",
      "Merging Ses03M_script01_1.pt\n",
      "Saved: /workspace/dataset/S3/features/Ses03M_script01_1_with_emo.pt (Matched 95, Missed 0)\n",
      "\n",
      "Merging Ses03M_script01_2.pt\n",
      "Saved: /workspace/dataset/S3/features/Ses03M_script01_2_with_emo.pt (Matched 38, Missed 0)\n",
      "\n",
      "Merging Ses03M_script01_3.pt\n",
      "Saved: /workspace/dataset/S3/features/Ses03M_script01_3_with_emo.pt (Matched 82, Missed 0)\n",
      "\n",
      "Merging Ses03M_script02_1.pt\n",
      "Saved: /workspace/dataset/S3/features/Ses03M_script02_1_with_emo.pt (Matched 68, Missed 0)\n",
      "\n",
      "Merging Ses03M_script02_2.pt\n",
      "Saved: /workspace/dataset/S3/features/Ses03M_script02_2_with_emo.pt (Matched 97, Missed 0)\n",
      "\n",
      "Merging Ses03M_script03_1.pt\n",
      "Saved: /workspace/dataset/S3/features/Ses03M_script03_1_with_emo.pt (Matched 71, Missed 0)\n",
      "\n",
      "Merging Ses03M_script03_2.pt\n",
      "Saved: /workspace/dataset/S3/features/Ses03M_script03_2_with_emo.pt (Matched 93, Missed 0)\n",
      "\n",
      "Merging Ses04F_impro01.pt\n",
      "Saved: /workspace/dataset/S4/features/Ses04F_impro01_with_emo.pt (Matched 49, Missed 0)\n",
      "\n",
      "Merging Ses04F_impro02.pt\n",
      "Saved: /workspace/dataset/S4/features/Ses04F_impro02_with_emo.pt (Matched 52, Missed 0)\n",
      "\n",
      "Merging Ses04F_impro03.pt\n",
      "Saved: /workspace/dataset/S4/features/Ses04F_impro03_with_emo.pt (Matched 104, Missed 0)\n",
      "\n",
      "Merging Ses04F_impro04.pt\n",
      "Saved: /workspace/dataset/S4/features/Ses04F_impro04_with_emo.pt (Matched 69, Missed 0)\n",
      "\n",
      "Merging Ses04F_impro05.pt\n",
      "Saved: /workspace/dataset/S4/features/Ses04F_impro05_with_emo.pt (Matched 45, Missed 0)\n",
      "\n",
      "Merging Ses04F_impro06.pt\n",
      "Saved: /workspace/dataset/S4/features/Ses04F_impro06_with_emo.pt (Matched 26, Missed 0)\n",
      "\n",
      "Merging Ses04F_impro07.pt\n",
      "Saved: /workspace/dataset/S4/features/Ses04F_impro07_with_emo.pt (Matched 167, Missed 0)\n",
      "\n",
      "Merging Ses04F_impro08.pt\n",
      "Saved: /workspace/dataset/S4/features/Ses04F_impro08_with_emo.pt (Matched 58, Missed 0)\n",
      "\n",
      "Merging Ses04F_script01_1.pt\n",
      "Saved: /workspace/dataset/S4/features/Ses04F_script01_1_with_emo.pt (Matched 91, Missed 0)\n",
      "\n",
      "Merging Ses04F_script01_3.pt\n",
      "Saved: /workspace/dataset/S4/features/Ses04F_script01_3_with_emo.pt (Matched 79, Missed 0)\n",
      "\n",
      "Merging Ses04F_script01_2.pt\n",
      "Saved: /workspace/dataset/S4/features/Ses04F_script01_2_with_emo.pt (Matched 41, Missed 0)\n",
      "\n",
      "Merging Ses04F_script02_2.pt\n",
      "Saved: /workspace/dataset/S4/features/Ses04F_script02_2_with_emo.pt (Matched 99, Missed 0)\n",
      "\n",
      "Merging Ses04F_script03_1.pt\n",
      "Saved: /workspace/dataset/S4/features/Ses04F_script03_1_with_emo.pt (Matched 68, Missed 0)\n",
      "\n",
      "Merging Ses04F_script03_2.pt\n",
      "Saved: /workspace/dataset/S4/features/Ses04F_script03_2_with_emo.pt (Matched 91, Missed 0)\n",
      "\n",
      "Merging Ses04M_impro01.pt\n",
      "Saved: /workspace/dataset/S4/features/Ses04M_impro01_with_emo.pt (Matched 47, Missed 0)\n",
      "\n",
      "Merging Ses04M_impro02.pt\n",
      "Saved: /workspace/dataset/S4/features/Ses04M_impro02_with_emo.pt (Matched 68, Missed 0)\n",
      "\n",
      "Merging Ses04M_impro03.pt\n",
      "Saved: /workspace/dataset/S4/features/Ses04M_impro03_with_emo.pt (Matched 58, Missed 0)\n",
      "\n",
      "Merging Ses04M_impro04.pt\n",
      "Saved: /workspace/dataset/S4/features/Ses04M_impro04_with_emo.pt (Matched 43, Missed 0)\n",
      "\n",
      "Merging Ses04M_impro05.pt\n",
      "Saved: /workspace/dataset/S4/features/Ses04M_impro05_with_emo.pt (Matched 89, Missed 0)\n",
      "\n",
      "Merging Ses04M_impro06.pt\n",
      "Saved: /workspace/dataset/S4/features/Ses04M_impro06_with_emo.pt (Matched 41, Missed 0)\n",
      "\n",
      "Merging Ses04F_script02_1.pt\n",
      "Saved: /workspace/dataset/S4/features/Ses04F_script02_1_with_emo.pt (Matched 66, Missed 0)\n",
      "\n",
      "Merging Ses04M_script01_1.pt\n",
      "Saved: /workspace/dataset/S4/features/Ses04M_script01_1_with_emo.pt (Matched 89, Missed 0)\n",
      "\n",
      "Merging Ses04M_script01_2.pt\n",
      "Saved: /workspace/dataset/S4/features/Ses04M_script01_2_with_emo.pt (Matched 41, Missed 0)\n",
      "\n",
      "Merging Ses04M_script01_3.pt\n",
      "Saved: /workspace/dataset/S4/features/Ses04M_script01_3_with_emo.pt (Matched 66, Missed 0)\n",
      "\n",
      "Merging Ses04M_impro08.pt\n",
      "Saved: /workspace/dataset/S4/features/Ses04M_impro08_with_emo.pt (Matched 60, Missed 0)\n",
      "\n",
      "Merging Ses04M_script02_2.pt\n",
      "Saved: /workspace/dataset/S4/features/Ses04M_script02_2_with_emo.pt (Matched 81, Missed 0)\n",
      "\n",
      "Merging Ses04M_script02_1.pt\n",
      "Saved: /workspace/dataset/S4/features/Ses04M_script02_1_with_emo.pt (Matched 56, Missed 0)\n",
      "\n",
      "Merging Ses04M_script03_1.pt\n",
      "Saved: /workspace/dataset/S4/features/Ses04M_script03_1_with_emo.pt (Matched 65, Missed 0)\n",
      "\n",
      "Merging Ses04M_script03_2.pt\n",
      "Saved: /workspace/dataset/S4/features/Ses04M_script03_2_with_emo.pt (Matched 110, Missed 0)\n",
      "\n",
      "Merging Ses04M_impro07.pt\n",
      "Saved: /workspace/dataset/S4/features/Ses04M_impro07_with_emo.pt (Matched 84, Missed 0)\n",
      "\n",
      "Merging Ses05F_impro01.pt\n",
      "No label for Ses05F_impro01_FXX0\n",
      "Saved: /workspace/dataset/S5/features/Ses05F_impro01_with_emo.pt (Matched 49, Missed 1)\n",
      "\n",
      "Merging Ses05F_impro02.pt\n",
      "No label for Ses05F_impro02_FXX0\n",
      "No label for Ses05F_impro02_MXX0\n",
      "Saved: /workspace/dataset/S5/features/Ses05F_impro02_with_emo.pt (Matched 81, Missed 2)\n",
      "\n",
      "Merging Ses05F_impro03.pt\n",
      "No label for Ses05F_impro03_FXX0\n",
      "No label for Ses05F_impro03_FXX1\n",
      "No label for Ses05F_impro03_MXX0\n",
      "No label for Ses05F_impro03_FXX2\n",
      "No label for Ses05F_impro03_FXX3\n",
      "Saved: /workspace/dataset/S5/features/Ses05F_impro03_with_emo.pt (Matched 130, Missed 5)\n",
      "\n",
      "Merging Ses05F_impro04.pt\n",
      "No label for Ses05F_impro04_MXX0\n",
      "Saved: /workspace/dataset/S5/features/Ses05F_impro04_with_emo.pt (Matched 91, Missed 1)\n",
      "\n",
      "Merging Ses05F_impro05.pt\n",
      "Saved: /workspace/dataset/S5/features/Ses05F_impro05_with_emo.pt (Matched 100, Missed 0)\n",
      "\n",
      "Merging Ses05F_impro06.pt\n",
      "Saved: /workspace/dataset/S5/features/Ses05F_impro06_with_emo.pt (Matched 56, Missed 0)\n",
      "\n",
      "Merging Ses05F_impro07.pt\n",
      "Saved: /workspace/dataset/S5/features/Ses05F_impro07_with_emo.pt (Matched 77, Missed 0)\n",
      "\n",
      "Merging Ses05F_impro08.pt\n",
      "No label for Ses05F_impro08_FXX0\n",
      "Saved: /workspace/dataset/S5/features/Ses05F_impro08_with_emo.pt (Matched 67, Missed 1)\n",
      "\n",
      "Merging Ses05F_script01_1.pt\n",
      "Saved: /workspace/dataset/S5/features/Ses05F_script01_1_with_emo.pt (Matched 83, Missed 0)\n",
      "\n",
      "Merging Ses05F_script01_2.pt\n",
      "Saved: /workspace/dataset/S5/features/Ses05F_script01_2_with_emo.pt (Matched 36, Missed 0)\n",
      "\n",
      "Merging Ses05F_script01_3.pt\n",
      "Saved: /workspace/dataset/S5/features/Ses05F_script01_3_with_emo.pt (Matched 68, Missed 0)\n",
      "\n",
      "Merging Ses05F_script02_1.pt\n",
      "No label for Ses05F_script02_1_FXX0\n",
      "Saved: /workspace/dataset/S5/features/Ses05F_script02_1_with_emo.pt (Matched 55, Missed 1)\n",
      "\n",
      "Merging Ses05F_script02_2.pt\n",
      "No label for Ses05F_script02_2_FXX0\n",
      "Saved: /workspace/dataset/S5/features/Ses05F_script02_2_with_emo.pt (Matched 81, Missed 1)\n",
      "\n",
      "Merging Ses05F_script03_1.pt\n",
      "Saved: /workspace/dataset/S5/features/Ses05F_script03_1_with_emo.pt (Matched 68, Missed 0)\n",
      "\n",
      "Merging Ses05F_script03_2.pt\n",
      "Saved: /workspace/dataset/S5/features/Ses05F_script03_2_with_emo.pt (Matched 86, Missed 0)\n",
      "\n",
      "Merging Ses05M_impro01.pt\n",
      "Saved: /workspace/dataset/S5/features/Ses05M_impro01_with_emo.pt (Matched 47, Missed 0)\n",
      "\n",
      "Merging Ses05M_impro02.pt\n",
      "No label for Ses05M_impro02_MXX0\n",
      "Saved: /workspace/dataset/S5/features/Ses05M_impro02_with_emo.pt (Matched 58, Missed 1)\n",
      "\n",
      "Merging Ses05M_impro03.pt\n",
      "No label for Ses05M_impro03_FXX0\n",
      "No label for Ses05M_impro03_FXX1\n",
      "Saved: /workspace/dataset/S5/features/Ses05M_impro03_with_emo.pt (Matched 66, Missed 2)\n",
      "\n",
      "Merging Ses05M_impro04.pt\n",
      "Saved: /workspace/dataset/S5/features/Ses05M_impro04_with_emo.pt (Matched 81, Missed 0)\n",
      "\n",
      "Merging Ses05M_impro05.pt\n",
      "Saved: /workspace/dataset/S5/features/Ses05M_impro05_with_emo.pt (Matched 45, Missed 0)\n",
      "\n",
      "Merging Ses05M_impro06.pt\n",
      "No label for Ses05M_impro06_FXX0\n",
      "No label for Ses05M_impro06_FXX1\n",
      "Saved: /workspace/dataset/S5/features/Ses05M_impro06_with_emo.pt (Matched 32, Missed 2)\n",
      "\n",
      "Merging Ses05M_impro07.pt\n",
      "No label for Ses05M_impro07_FXX1\n",
      "No label for Ses05M_impro07_FXX2\n",
      "No label for Ses05M_impro07_FXX3\n",
      "Saved: /workspace/dataset/S5/features/Ses05M_impro07_with_emo.pt (Matched 95, Missed 3)\n",
      "\n",
      "Merging Ses05M_impro08.pt\n",
      "No label for Ses05M_impro08_MXX0\n",
      "No label for Ses05M_impro08_FXX0\n",
      "Saved: /workspace/dataset/S5/features/Ses05M_impro08_with_emo.pt (Matched 58, Missed 2)\n",
      "\n",
      "Merging Ses05M_script01_1.pt\n",
      "Saved: /workspace/dataset/S5/features/Ses05M_script01_1_with_emo.pt (Matched 80, Missed 0)\n",
      "\n",
      "Merging Ses05M_script01_1b.pt\n",
      "Saved: /workspace/dataset/S5/features/Ses05M_script01_1b_with_emo.pt (Matched 81, Missed 0)\n",
      "\n",
      "Merging Ses05M_script01_2.pt\n",
      "Saved: /workspace/dataset/S5/features/Ses05M_script01_2_with_emo.pt (Matched 35, Missed 0)\n",
      "\n",
      "Merging Ses05M_script01_3.pt\n",
      "Saved: /workspace/dataset/S5/features/Ses05M_script01_3_with_emo.pt (Matched 71, Missed 0)\n",
      "\n",
      "Merging Ses05M_script02_1.pt\n",
      "Saved: /workspace/dataset/S5/features/Ses05M_script02_1_with_emo.pt (Matched 55, Missed 0)\n",
      "\n",
      "Merging Ses05M_script02_2.pt\n",
      "No label for Ses05M_script02_2_FXX0\n",
      "Saved: /workspace/dataset/S5/features/Ses05M_script02_2_with_emo.pt (Matched 80, Missed 1)\n",
      "\n",
      "Merging Ses05M_script03_1.pt\n",
      "Saved: /workspace/dataset/S5/features/Ses05M_script03_1_with_emo.pt (Matched 67, Missed 0)\n",
      "\n",
      "Merging Ses05M_script03_2.pt\n",
      "Saved: /workspace/dataset/S5/features/Ses05M_script03_2_with_emo.pt (Matched 91, Missed 0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "\n",
    "def parse_large_txt_strict(txt_path):\n",
    "    \"\"\"\n",
    "    严格解析 .txt 文件，确保 ID 一一对应\n",
    "    返回 dict: {utt_id: {...info...}}\n",
    "    \"\"\"\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    data = {}\n",
    "    current_id = None\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        # 识别 utterance 的头行\n",
    "        header = re.match(r\"\\[(.*?) - (.*?)\\]\\s+(\\S+)\\s+\\S+\\s+\\[(.*?)\\]\", line)\n",
    "        if header:\n",
    "            start, end, utt_id, vad_str = header.groups()\n",
    "            vad = [float(x.strip()) for x in vad_str.split(\",\")]\n",
    "            data[utt_id] = {\n",
    "                \"start_time\": float(start),\n",
    "                \"end_time\": float(end),\n",
    "                \"VAD\": vad,\n",
    "                \"C-E1\": None,\n",
    "                \"C-E2\": None,\n",
    "                \"C-E4\": None,\n",
    "            }\n",
    "            current_id = utt_id\n",
    "            continue\n",
    "\n",
    "        # 在 block 内读取 C-E1/C-E2/C-E4\n",
    "        if current_id:\n",
    "            for tag in [\"C-E1\", \"C-E2\", \"C-E4\"]:\n",
    "                if line.startswith(f\"{tag}:\"):\n",
    "                    emo = line.split(\":\")[1].split(\";\")[0].strip()\n",
    "                    data[current_id][tag] = emo\n",
    "    return data\n",
    "\n",
    "\n",
    "def merge_with_pt_strict(pt_path, txt_path):\n",
    "    \"\"\"\n",
    "    严格根据 utterance ID 对齐并合并标注信息\n",
    "    \"\"\"\n",
    "    print(f\"Merging {os.path.basename(pt_path)}\")\n",
    "\n",
    "    info_dict = parse_large_txt_strict(txt_path)\n",
    "    data = torch.load(pt_path, map_location=\"cpu\")\n",
    "\n",
    "    matched, missed = 0, 0\n",
    "    for utt_id, sample in data.items():\n",
    "        if utt_id in info_dict:\n",
    "            sample.update(info_dict[utt_id])\n",
    "            matched += 1\n",
    "        else:\n",
    "            missed += 1\n",
    "            print(f\"No label for {utt_id}\")\n",
    "\n",
    "    save_path = pt_path.replace(\".pt\", \"_with_emo.pt\")\n",
    "    torch.save(data, save_path)\n",
    "    print(f\"Saved: {save_path} (Matched {matched}, Missed {missed})\\n\")\n",
    "\n",
    "\n",
    "def batch_process(base_dir=\"/workspace/dataset\"):\n",
    "    for s in [\"S1\", \"S2\", \"S3\", \"S4\", \"S5\"]:\n",
    "        feat_dir = os.path.join(base_dir, s, \"features\")\n",
    "        txt_dir = os.path.join(base_dir, s, \"LargeWords\")\n",
    "\n",
    "        if not os.path.exists(feat_dir) or not os.path.exists(txt_dir):\n",
    "            continue\n",
    "\n",
    "        for fname in os.listdir(feat_dir):\n",
    "            if not fname.endswith(\".pt\"):\n",
    "                continue\n",
    "            pt_path = os.path.join(feat_dir, fname)\n",
    "            base_name = fname.replace(\".pt\", \"\")\n",
    "            txt_path = os.path.join(txt_dir, f\"{base_name}.txt\")\n",
    "\n",
    "            if os.path.exists(txt_path):\n",
    "                merge_with_pt_strict(pt_path, txt_path)\n",
    "            else:\n",
    "                print(f\"Missing txt: {txt_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    batch_process(\"/workspace/dataset\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d8a177-976b-4d22-9168-becbcddc02ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

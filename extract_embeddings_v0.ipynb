{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5dc88c1-39e9-4ef9-b788-fe5deeeee0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-1.0.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2023.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (0.28.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (6.0.1)\n",
      "Collecting shellingham (from huggingface_hub)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.65.0)\n",
      "Collecting typer-slim (from huggingface_hub)\n",
      "  Downloading typer_slim-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.15.0)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface_hub)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface_hub) (4.11.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface_hub) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface_hub) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface_hub) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub) (0.16.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from typer-slim->huggingface_hub) (8.1.7)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->huggingface_hub) (1.2.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->huggingface_hub) (1.3.1)\n",
      "Downloading huggingface_hub-1.0.0-py3-none-any.whl (503 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m503.9/503.9 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m107.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading typer_slim-0.20.0-py3-none-any.whl (47 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: typer-slim, shellingham, hf-xet, huggingface_hub\n",
      "Successfully installed hf-xet-1.2.0 huggingface_hub-1.0.0 shellingham-1.5.4 typer-slim-0.20.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "404ebeb8-a1a4-402c-8e25-cc157528cc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting safetensors\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.8/485.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors\n",
      "Successfully installed safetensors-0.6.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install safetensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e7050ae-a5ec-486f-8c8b-f6b6e7f458f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch 已安装\n",
      " torchvision 已安装\n",
      " torchaudio 已安装\n",
      " tqdm 已安装\n",
      " numpy 已安装\n",
      " soundfile 已安装\n",
      " librosa 已安装\n",
      " 缺少 opencv-python，请安装：pip install opencv-python\n",
      " 缺少 decord，请安装：pip install decord\n",
      " 缺少 imageio，请安装：pip install imageio\n",
      " ftfy 已安装\n",
      " 缺少 sentencepiece，请安装：pip install sentencepiece\n",
      " huggingface_hub 已安装\n",
      " safetensors 已安装\n",
      " timm 已安装\n",
      " einops 已安装\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "required = [\n",
    "    \"torch\", \"torchvision\", \"torchaudio\",\n",
    "    \"tqdm\", \"numpy\", \"soundfile\", \"librosa\",\n",
    "    \"opencv-python\", \"decord\", \"imageio\", \"ftfy\",\n",
    "    \"sentencepiece\", \"huggingface_hub\", \"safetensors\", \"timm\", \"einops\"\n",
    "]\n",
    "\n",
    "for pkg in required:\n",
    "    try:\n",
    "        importlib.import_module(pkg.split('-')[0])  # 兼容opencv-python这种命名\n",
    "        print(f\" {pkg} 已安装\")\n",
    "    except ImportError:\n",
    "        print(f\" 缺少 {pkg}，请安装：pip install {pkg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47330b85-ae25-4a01-8b97-cdb5ef2946ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target session: Session1\n",
      "AVI folder: /workspace/dataset/IEMOCAP/Session1/Session1/dialog/avi/DivX\n",
      "FFmpeg found: ffmpeg version 4.3 Copyright (c) 2000-2020 the FFmpeg developers\n",
      "Removed 0 hidden/.ico files in Session1\n",
      "Found 28 .avi files to convert.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting AVI → MP4:   0%|          | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping (already exists): Ses01F_impro01.mp4\n",
      "Skipping (already exists): Ses01F_impro02.mp4\n",
      "Skipping (already exists): Ses01F_impro03.mp4\n",
      "Skipping (already exists): Ses01F_impro04.mp4\n",
      "Skipping (already exists): Ses01F_impro05.mp4\n",
      "Skipping (already exists): Ses01F_impro06.mp4\n",
      "Skipping (already exists): Ses01F_impro07.mp4\n",
      "Skipping (already exists): Ses01F_script01_1.mp4\n",
      "Skipping (already exists): Ses01F_script01_2.mp4\n",
      "Skipping (already exists): Ses01F_script01_3.mp4\n",
      "Skipping (already exists): Ses01F_script02_1.mp4\n",
      "Skipping (already exists): Ses01F_script02_2.mp4\n",
      "Skipping (already exists): Ses01F_script03_1.mp4\n",
      "Skipping (already exists): Ses01F_script03_2.mp4\n",
      "Skipping (already exists): Ses01M_impro01.mp4\n",
      "Skipping (already exists): Ses01M_impro02.mp4\n",
      "Skipping (already exists): Ses01M_impro03.mp4\n",
      "Converting: Ses01M_impro04.avi ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting AVI → MP4:  64%|██████▍   | 18/28 [00:05<00:03,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Ses01M_impro04.mp4\n",
      "Deleted original AVI: Ses01M_impro04.avi\n",
      "Converting: Ses01M_impro05.avi ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting AVI → MP4:  68%|██████▊   | 19/28 [00:11<00:06,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Ses01M_impro05.mp4\n",
      "Deleted original AVI: Ses01M_impro05.avi\n",
      "Converting: Ses01M_impro06.avi ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting AVI → MP4:  71%|███████▏  | 20/28 [00:19<00:11,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Ses01M_impro06.mp4\n",
      "Deleted original AVI: Ses01M_impro06.avi\n",
      "Converting: Ses01M_impro07.avi ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting AVI → MP4:  75%|███████▌  | 21/28 [00:25<00:13,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Ses01M_impro07.mp4\n",
      "Deleted original AVI: Ses01M_impro07.avi\n",
      "Converting: Ses01M_script01_1.avi ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting AVI → MP4:  79%|███████▊  | 22/28 [00:38<00:21,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Ses01M_script01_1.mp4\n",
      "Deleted original AVI: Ses01M_script01_1.avi\n",
      "Converting: Ses01M_script01_2.avi ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting AVI → MP4:  82%|████████▏ | 23/28 [00:43<00:18,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Ses01M_script01_2.mp4\n",
      "Deleted original AVI: Ses01M_script01_2.avi\n",
      "Converting: Ses01M_script01_3.avi ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting AVI → MP4:  86%|████████▌ | 24/28 [00:58<00:24,  6.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Ses01M_script01_3.mp4\n",
      "Deleted original AVI: Ses01M_script01_3.avi\n",
      "Converting: Ses01M_script02_1.avi ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting AVI → MP4:  89%|████████▉ | 25/28 [01:10<00:22,  7.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Ses01M_script02_1.mp4\n",
      "Deleted original AVI: Ses01M_script02_1.avi\n",
      "Converting: Ses01M_script02_2.avi ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting AVI → MP4:  93%|█████████▎| 26/28 [01:27<00:19,  9.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Ses01M_script02_2.mp4\n",
      "Deleted original AVI: Ses01M_script02_2.avi\n",
      "Converting: Ses01M_script03_1.avi ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting AVI → MP4:  96%|█████████▋| 27/28 [01:36<00:09,  9.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Ses01M_script03_1.mp4\n",
      "Deleted original AVI: Ses01M_script03_1.avi\n",
      "Converting: Ses01M_script03_2.avi ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting AVI → MP4: 100%|██████████| 28/28 [01:46<00:00,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Ses01M_script03_2.mp4\n",
      "Deleted original AVI: Ses01M_script03_2.avi\n",
      "\n",
      "===========================\n",
      "Conversion finished!\n",
      "Converted: 11/28\n",
      "AVI deleted: 11\n",
      "===========================\n",
      "\n",
      "Step 1 & 2 complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =====================================================\n",
    "# 路径配置\n",
    "# =====================================================\n",
    "BASE_PATH = \"/workspace/dataset/IEMOCAP\"\n",
    "SESSION = \"Session1\"\n",
    "\n",
    "session_path = os.path.join(BASE_PATH, SESSION, SESSION)\n",
    "dialog_path = os.path.join(session_path, \"dialog\")\n",
    "avi_path = os.path.join(dialog_path, \"avi\", \"DivX\")\n",
    "\n",
    "print(f\"Target session: {SESSION}\")\n",
    "print(f\"AVI folder: {avi_path}\")\n",
    "\n",
    "# =====================================================\n",
    "# 检查 ffmpeg 是否可用\n",
    "# =====================================================\n",
    "try:\n",
    "    result = subprocess.run([\"ffmpeg\", \"-version\"], capture_output=True, text=True)\n",
    "    print(\"FFmpeg found:\", result.stdout.split(\"\\n\")[0])\n",
    "except Exception as e:\n",
    "    print(\"FFmpeg not found or not executable:\", e)\n",
    "    raise SystemExit\n",
    "\n",
    "# =====================================================\n",
    "# 删除隐藏文件和\n",
    "# =====================================================\n",
    "def remove_hidden_and_ico_files(root_path):\n",
    "    deleted = 0\n",
    "    for root, _, files in os.walk(root_path):\n",
    "        for f in files:\n",
    "            if f.startswith(\".\") or f.startswith(\"._\") or f.lower().endswith(\".ico\"):\n",
    "                try:\n",
    "                    os.remove(os.path.join(root, f))\n",
    "                    deleted += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to delete {f}: {e}\")\n",
    "    return deleted\n",
    "\n",
    "total_removed = remove_hidden_and_ico_files(avi_path)\n",
    "print(f\"Removed {total_removed} hidden/.ico files in {SESSION}\")\n",
    "\n",
    "# =====================================================\n",
    "# 转换函数：.avi → .mp4（成功后自动删除 .avi）\n",
    "# =====================================================\n",
    "def convert_avi_to_mp4(avi_file):\n",
    "    mp4_path = avi_file.replace(\".avi\", \".mp4\")\n",
    "\n",
    "    # 跳过已存在\n",
    "    if os.path.exists(mp4_path):\n",
    "        print(f\"Skipping (already exists): {os.path.basename(mp4_path)}\")\n",
    "        return False\n",
    "\n",
    "    cmd = [\n",
    "        \"ffmpeg\", \"-y\", \"-i\", avi_file,\n",
    "        \"-c:v\", \"mpeg4\",\n",
    "        \"-qscale:v\", \"3\",\n",
    "        \"-pix_fmt\", \"yuv420p\",\n",
    "        \"-threads\", \"1\",\n",
    "        \"-loglevel\", \"error\",\n",
    "        mp4_path\n",
    "    ]\n",
    "\n",
    "    print(f\"Converting: {os.path.basename(avi_file)} ...\")\n",
    "    try:\n",
    "        subprocess.run(cmd, check=True)\n",
    "\n",
    "        if os.path.exists(mp4_path) and os.path.getsize(mp4_path) > 1000000:  # 至少1MB\n",
    "            print(f\"Success: {os.path.basename(mp4_path)}\")\n",
    "\n",
    "            # 删除原始 .avi 文件\n",
    "            try:\n",
    "                os.remove(avi_file)\n",
    "                print(f\"Deleted original AVI: {os.path.basename(avi_file)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to delete AVI: {avi_file}, reason: {e}\")\n",
    "\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Conversion failed or output too small: {os.path.basename(mp4_path)}\")\n",
    "            return False\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"FFmpeg error for {avi_file}: {e}\")\n",
    "        return False\n",
    "\n",
    "# =====================================================\n",
    "# 执行批量转码 + 删除\n",
    "# =====================================================\n",
    "if not os.path.exists(avi_path):\n",
    "    print(f\"AVI folder not found: {avi_path}\")\n",
    "else:\n",
    "    avi_files = [f for f in os.listdir(avi_path) if f.endswith(\".avi\") and not f.startswith(\".\")]\n",
    "    print(f\"Found {len(avi_files)} .avi files to convert.\\n\")\n",
    "\n",
    "    converted = 0\n",
    "    deleted = 0\n",
    "\n",
    "    for f in tqdm(avi_files, desc=\"Converting AVI → MP4\"):\n",
    "        full_path = os.path.join(avi_path, f)\n",
    "        ok = convert_avi_to_mp4(full_path)\n",
    "        if ok:\n",
    "            converted += 1\n",
    "            deleted += 1\n",
    "\n",
    "    print(\"\\n===========================\")\n",
    "    print(\"Conversion finished!\")\n",
    "    print(f\"Converted: {converted}/{len(avi_files)}\")\n",
    "    print(f\"AVI deleted: {deleted}\")\n",
    "    print(\"===========================\\n\")\n",
    "\n",
    "print(\"Step 1 & 2 complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "763049e4-5b92-4eaf-9f4b-a110684fffd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "#  IEMOCAP + ImageBind Multimodal Embedding Extraction\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from imagebind.models import imagebind_model\n",
    "from imagebind import data\n",
    "import safetensors\n",
    "from safetensors import torch as safetorch \n",
    "sys.modules[\"safetensors\"] = safetensors\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ec2617-03ba-4ce9-aca8-68aa8f0c0677",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "\n",
      "Processing Session1 ...\n",
      "GPU Memory - Allocated: 0.00 GB | Reserved: 0.00 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Session1 audio files:   0%|          | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting embedding extraction...\n",
      "GPU Memory - Allocated: 0.00 GB | Reserved: 0.00 GB\n",
      "Skipping video /workspace/dataset/IEMOCAP/Session1/Session1/dialog/avi/DivX/Ses01F_impro01.mp4: load_and_transform_vision_data() got an unexpected keyword argument 'num_frames'\n"
     ]
    }
   ],
   "source": [
    "import os, torch, gc, time\n",
    "from tqdm import tqdm\n",
    "from imagebind.models import imagebind_model\n",
    "from safetensors.torch import load_file\n",
    "from imagebind import data\n",
    "\n",
    "# =====================================================\n",
    "# 配置区域\n",
    "# =====================================================\n",
    "BASE_PATH = \"/workspace/dataset/IEMOCAP\"\n",
    "OUTPUT_DIR = os.path.join(BASE_PATH, \"embeddings\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 模态开关（方便 debug）\n",
    "USE_TEXT = True\n",
    "USE_AUDIO = True\n",
    "USE_VIDEO = True   \n",
    "\n",
    "# =====================================================\n",
    "# 显存监控与清理工具\n",
    "# =====================================================\n",
    "def gpu_mem():\n",
    "    if torch.cuda.is_available():\n",
    "        alloc = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserv = torch.cuda.memory_reserved() / 1024**3\n",
    "        print(f\"GPU Memory - Allocated: {alloc:.2f} GB | Reserved: {reserv:.2f} GB\")\n",
    "    else:\n",
    "        print(\"No CUDA available\")\n",
    "\n",
    "def clear_gpu():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    time.sleep(0.5)\n",
    "    gpu_mem()\n",
    "\n",
    "# =====================================================\n",
    "# 提取单样本 embedding（仅使用 mp4）\n",
    "# =====================================================\n",
    "def extract_embedding(text_path=None, audio_path=None, video_path=None):\n",
    "    try:\n",
    "        print(\"Starting embedding extraction...\")\n",
    "        gpu_mem()\n",
    "\n",
    "        # ---- 加载模型 ----\n",
    "        model = imagebind_model.imagebind_huge(pretrained=False).to(device)\n",
    "        weight_path = \"/ImageBind/model.safetensors\"\n",
    "        state_dict = load_file(weight_path, device=device)\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "        model.eval()\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        inputs = {}\n",
    "\n",
    "        # ---- 文本 ----\n",
    "        if USE_TEXT and text_path and os.path.exists(text_path):\n",
    "            with open(text_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read().strip()\n",
    "            if text:\n",
    "                inputs[imagebind_model.ModalityType.TEXT] = data.load_and_transform_text([text], device)\n",
    "\n",
    "        # ---- 音频 ----\n",
    "        if USE_AUDIO and audio_path and os.path.exists(audio_path):\n",
    "            inputs[imagebind_model.ModalityType.AUDIO] = data.load_and_transform_audio_data([audio_path], device)\n",
    "\n",
    "        # ---- 视频 (直接读 mp4，不再调用 ffmpeg) ----\n",
    "        if USE_VIDEO and video_path and os.path.exists(video_path):\n",
    "            try:\n",
    "                inputs[imagebind_model.ModalityType.VISION] = data.load_and_transform_vision_data(\n",
    "                    [video_path], device, num_frames=2\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping video {video_path}: {e}\")\n",
    "\n",
    "        if not inputs:\n",
    "            print(\"No valid inputs found for this sample.\")\n",
    "            del model\n",
    "            clear_gpu()\n",
    "            return None\n",
    "\n",
    "        # ---- 提取特征 ----\n",
    "        with torch.no_grad():\n",
    "            embedding = model(inputs)\n",
    "\n",
    "        # ---- 融合多模态 ----\n",
    "        embed_list = [v for v in embedding.values()]\n",
    "        fused = torch.stack(embed_list).mean(dim=0)\n",
    "\n",
    "        # ---- 清理显存 ----\n",
    "        del model, embedding, inputs\n",
    "        clear_gpu()\n",
    "\n",
    "        return fused.cpu()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fatal error in extract_embedding: {e}\")\n",
    "        clear_gpu()\n",
    "        return None\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 文件过滤函数\n",
    "# =====================================================\n",
    "def is_valid_file(filename):\n",
    "    return not (filename.startswith(\".\") or filename.startswith(\"._\"))\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 主循环：遍历所有 Session\n",
    "# =====================================================\n",
    "# sessions = [f\"Session{i}\" for i in range(1, 6)]\n",
    "sessions = [\"Session1\"]\n",
    "\n",
    "for session in sessions:\n",
    "    print(f\"\\nProcessing {session} ...\")\n",
    "    clear_gpu()\n",
    "\n",
    "    session_path = os.path.join(BASE_PATH, session, session)\n",
    "    dialog_path = os.path.join(session_path, \"dialog\")\n",
    "\n",
    "    wav_path = os.path.join(dialog_path, \"wav\")\n",
    "    text_path = os.path.join(dialog_path, \"transcriptions\")\n",
    "    avi_path = os.path.join(dialog_path, \"avi\", \"DivX\")  # 注意：这里仍是视频路径，只是文件改为 .mp4\n",
    "\n",
    "    if not os.path.exists(wav_path):\n",
    "        print(f\"Missing wav directory in {session}\")\n",
    "        continue\n",
    "\n",
    "    embeddings = {}\n",
    "\n",
    "    for root, _, files in os.walk(wav_path):\n",
    "        for file in tqdm(files, desc=f\"{session} audio files\"):\n",
    "            if not is_valid_file(file) or not file.endswith(\".wav\"):\n",
    "                continue\n",
    "\n",
    "            base_id = os.path.splitext(file)[0]\n",
    "            wav_file = os.path.join(root, file)\n",
    "            txt_file = os.path.join(text_path, f\"{base_id}.txt\")\n",
    "            mp4_file = os.path.join(avi_path, f\"{base_id}.mp4\")  # 直接读取 mp4 文件\n",
    "\n",
    "            try:\n",
    "                emb = extract_embedding(text_path=txt_file, audio_path=wav_file, video_path=mp4_file)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {base_id}: {e}\")\n",
    "                continue\n",
    "\n",
    "            if emb is None:\n",
    "                continue\n",
    "\n",
    "            embeddings[base_id] = {\n",
    "                \"text_path\": txt_file if os.path.exists(txt_file) else None,\n",
    "                \"audio_path\": wav_file,\n",
    "                \"video_path\": mp4_file if os.path.exists(mp4_file) else None,\n",
    "                \"embedding\": emb.squeeze(0),\n",
    "            }\n",
    "\n",
    "            # 每5条保存一次临时结果\n",
    "            if len(embeddings) % 5 == 0:\n",
    "                temp_path = os.path.join(OUTPUT_DIR, f\"{session}_partial.pt\")\n",
    "                torch.save(embeddings, temp_path)\n",
    "                print(f\"Saved checkpoint: {temp_path}\")\n",
    "                clear_gpu()\n",
    "\n",
    "    out_path = os.path.join(OUTPUT_DIR, f\"{session}_embeddings.pt\")\n",
    "    torch.save(embeddings, out_path)\n",
    "    print(f\"Saved {len(embeddings)} embeddings to {out_path}\")\n",
    "\n",
    "    del embeddings\n",
    "    clear_gpu()\n",
    "\n",
    "print(\"\\nAll sessions processed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "734abe2c-c63b-45f6-b32e-dfd4742bcb10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Using device: cuda:0\n",
      "  All model weights loaded successfully!\n",
      " ImageBind model loaded successfully!\n",
      "All ImageBind parameters are frozen.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# 基本路径设置\n",
    "# --------------------------------------------\n",
    "BASE_PATH = \"/workspace/dataset/IEMOCAP\"\n",
    "OUTPUT_DIR = os.path.join(BASE_PATH, \"embeddings\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --------------------------------------------\n",
    "# 设备配置\n",
    "# --------------------------------------------\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\" Using device: {device}\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# 加载 ImageBind 模型\n",
    "# --------------------------------------------\n",
    "# 1️ 初始化空模型结构\n",
    "model = imagebind_model.imagebind_huge(pretrained=False)\n",
    "\n",
    "\n",
    "# 2️ 加载权重（直接读取 safetensors 文件）\n",
    "weight_path = \"/ImageBind/model.safetensors\"  #  safetensors 文件路径\n",
    "state_dict = safetorch.load_file(weight_path) \n",
    "\n",
    "# 3️ 将权重加载进模型\n",
    "missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "# 打印加载情况\n",
    "if missing_keys:\n",
    "    print(f\"  Missing keys: {len(missing_keys)} (部分权重可能未匹配)\")\n",
    "if unexpected_keys:\n",
    "    print(f\"  Unexpected keys: {len(unexpected_keys)} (模型结构有少量差异)\")\n",
    "if not missing_keys and not unexpected_keys:\n",
    "    print(\"  All model weights loaded successfully!\")\n",
    "\n",
    "model.eval().to(device)\n",
    "print(\" ImageBind model loaded successfully!\")\n",
    "\n",
    "# 冻结所有参数\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"All ImageBind parameters are frozen.\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# 文件过滤函数：跳过所有 macOS 垃圾文件\n",
    "# --------------------------------------------\n",
    "def is_valid_file(filename):\n",
    "    if filename.startswith(\".\") or filename.startswith(\"._\"):\n",
    "        return False\n",
    "    return True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d254706-34e6-4187-b9ad-861aecea7fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook 仍在运行，输出刷新正常\n"
     ]
    }
   ],
   "source": [
    "print(\"Notebook 仍在运行，输出刷新正常\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f72cdb72-e963-4a61-a7c0-67dcca90fadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Processing Session1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Session1 audio files:  52%|█████▏    | 29/56 [00:11<00:11,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Error processing Ses01F_impro01: could not find MARK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Session1 audio files:  54%|█████▎    | 30/56 [00:22<00:23,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Error processing Ses01F_impro02: could not find MARK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Session1 audio files:  55%|█████▌    | 31/56 [00:33<00:37,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Error processing Ses01F_impro03: could not find MARK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Session1 audio files:  57%|█████▋    | 32/56 [00:43<00:53,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Error processing Ses01F_impro04: could not find MARK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Session1 audio files:  59%|█████▉    | 33/56 [00:53<01:10,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Error processing Ses01F_impro05: could not find MARK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Session1 audio files:  61%|██████    | 34/56 [01:04<01:28,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Error processing Ses01F_impro06: could not find MARK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Session1 audio files:  62%|██████▎   | 35/56 [01:14<01:44,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Error processing Ses01F_impro07: could not find MARK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Session1 audio files:  64%|██████▍   | 36/56 [01:24<01:58,  5.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Error processing Ses01F_script01_1: could not find MARK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Session1 audio files:  66%|██████▌   | 37/56 [01:34<02:09,  6.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Error processing Ses01F_script01_2: could not find MARK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Session1 audio files:  68%|██████▊   | 38/56 [01:44<02:16,  7.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Error processing Ses01F_script01_3: could not find MARK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Session1 audio files:  70%|██████▉   | 39/56 [01:55<02:19,  8.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Error processing Ses01F_script02_1: could not find MARK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Session1 audio files:  71%|███████▏  | 40/56 [02:05<02:19,  8.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Error processing Ses01F_script02_2: could not find MARK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Session1 audio files:  73%|███████▎  | 41/56 [02:15<02:17,  9.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Error processing Ses01F_script03_1: could not find MARK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Session1 audio files:  75%|███████▌  | 42/56 [02:25<02:11,  9.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Error processing Ses01F_script03_2: could not find MARK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Session1 audio files:  77%|███████▋  | 43/56 [02:35<02:05,  9.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Error processing Ses01M_impro01: could not find MARK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Session1 audio files:  79%|███████▊  | 44/56 [02:45<01:57,  9.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Error processing Ses01M_impro02: could not find MARK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Session1 audio files:  80%|████████  | 45/56 [02:56<01:48,  9.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Error processing Ses01M_impro03: could not find MARK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Session1 audio files:  82%|████████▏ | 46/56 [03:06<01:39,  9.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Error processing Ses01M_impro04: could not find MARK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Session1 audio files:  84%|████████▍ | 47/56 [03:16<01:30, 10.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Error processing Ses01M_impro05: could not find MARK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Session1 audio files:  86%|████████▌ | 48/56 [03:26<01:20, 10.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Error processing Ses01M_impro06: could not find MARK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Session1 audio files:  88%|████████▊ | 49/56 [03:36<01:10, 10.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Error processing Ses01M_impro07: could not find MARK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Session1 audio files:  89%|████████▉ | 50/56 [03:47<01:00, 10.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Error processing Ses01M_script01_1: could not find MARK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Session1 audio files:  91%|█████████ | 51/56 [03:57<00:50, 10.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Error processing Ses01M_script01_2: could not find MARK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Session1 audio files:  93%|█████████▎| 52/56 [04:07<00:40, 10.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Error processing Ses01M_script01_3: could not find MARK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Session1 audio files:  95%|█████████▍| 53/56 [04:17<00:30, 10.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Error processing Ses01M_script02_1: could not find MARK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Session1 audio files:  96%|█████████▋| 54/56 [04:27<00:20, 10.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Error processing Ses01M_script02_2: could not find MARK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Session1 audio files:  96%|█████████▋| 54/56 [04:36<00:10,  5.11s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 88\u001b[0m\n\u001b[1;32m     85\u001b[0m avi_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(avi_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.avi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 88\u001b[0m     emb \u001b[38;5;241m=\u001b[39m \u001b[43mextract_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtxt_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwav_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mavi_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Error processing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m, in \u001b[0;36mextract_embedding\u001b[0;34m(text_path, audio_path, video_path)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_embedding\u001b[39m(text_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, audio_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, video_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m#  每次都加载模型（不会占用太久显存）\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mimagebind_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimagebind_huge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m     weight_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/ImageBind/model.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(weight_path, map_location\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m/ImageBind/imagebind/models/imagebind_model.py:482\u001b[0m, in \u001b[0;36mimagebind_huge\u001b[0;34m(pretrained)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimagebind_huge\u001b[39m(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 482\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mImageBindModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvision_embed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1280\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvision_num_blocks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvision_num_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_embed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_num_blocks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_num_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mout_embed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43maudio_drop_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimu_drop_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pretrained:\n\u001b[1;32m    495\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.checkpoints/imagebind_huge.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/ImageBind/imagebind/models/imagebind_model.py:94\u001b[0m, in \u001b[0;36mImageBindModel.__init__\u001b[0;34m(self, video_frames, kernel_size, audio_kernel_size, audio_stride, out_embed_dim, vision_embed_dim, vision_num_blocks, vision_num_heads, audio_embed_dim, audio_num_blocks, audio_num_heads, audio_num_mel_bins, audio_target_len, audio_drop_path, text_embed_dim, text_num_blocks, text_num_heads, depth_embed_dim, depth_kernel_size, depth_num_blocks, depth_num_heads, depth_drop_path, thermal_embed_dim, thermal_kernel_size, thermal_num_blocks, thermal_num_heads, thermal_drop_path, imu_embed_dim, imu_kernel_size, imu_num_blocks, imu_num_heads, imu_drop_path)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodality_preprocessors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_modality_preprocessors(\n\u001b[1;32m     78\u001b[0m     video_frames,\n\u001b[1;32m     79\u001b[0m     vision_embed_dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     91\u001b[0m     imu_embed_dim,\n\u001b[1;32m     92\u001b[0m )\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodality_trunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_modality_trunks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvision_embed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvision_num_blocks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvision_num_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_embed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_num_blocks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_num_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43maudio_embed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43maudio_num_blocks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43maudio_num_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43maudio_drop_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdepth_embed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdepth_num_blocks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdepth_num_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdepth_drop_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthermal_embed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthermal_num_blocks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthermal_num_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthermal_drop_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimu_embed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimu_num_blocks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimu_num_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimu_drop_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodality_heads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_modality_heads(\n\u001b[1;32m    120\u001b[0m     out_embed_dim,\n\u001b[1;32m    121\u001b[0m     vision_embed_dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    126\u001b[0m     imu_embed_dim,\n\u001b[1;32m    127\u001b[0m )\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodality_postprocessors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_modality_postprocessors(\n\u001b[1;32m    130\u001b[0m     out_embed_dim\n\u001b[1;32m    131\u001b[0m )\n",
      "File \u001b[0;32m/ImageBind/imagebind/models/imagebind_model.py:349\u001b[0m, in \u001b[0;36mImageBindModel._create_modality_trunks\u001b[0;34m(self, vision_embed_dim, vision_num_blocks, vision_num_heads, text_embed_dim, text_num_blocks, text_num_heads, audio_embed_dim, audio_num_blocks, audio_num_heads, audio_drop_path, depth_embed_dim, depth_num_blocks, depth_num_heads, depth_drop_path, thermal_embed_dim, thermal_num_blocks, thermal_num_heads, thermal_drop_path, imu_embed_dim, imu_num_blocks, imu_num_heads, imu_drop_path)\u001b[0m\n\u001b[1;32m    333\u001b[0m modality_trunks[ModalityType\u001b[38;5;241m.\u001b[39mAUDIO] \u001b[38;5;241m=\u001b[39m instantiate_trunk(\n\u001b[1;32m    334\u001b[0m     audio_embed_dim,\n\u001b[1;32m    335\u001b[0m     audio_num_blocks,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    339\u001b[0m     drop_path\u001b[38;5;241m=\u001b[39maudio_drop_path,\n\u001b[1;32m    340\u001b[0m )\n\u001b[1;32m    341\u001b[0m modality_trunks[ModalityType\u001b[38;5;241m.\u001b[39mDEPTH] \u001b[38;5;241m=\u001b[39m instantiate_trunk(\n\u001b[1;32m    342\u001b[0m     depth_embed_dim,\n\u001b[1;32m    343\u001b[0m     depth_num_blocks,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    347\u001b[0m     drop_path\u001b[38;5;241m=\u001b[39mdepth_drop_path,\n\u001b[1;32m    348\u001b[0m )\n\u001b[0;32m--> 349\u001b[0m modality_trunks[ModalityType\u001b[38;5;241m.\u001b[39mTHERMAL] \u001b[38;5;241m=\u001b[39m \u001b[43minstantiate_trunk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthermal_embed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthermal_num_blocks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthermal_num_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_transformer_ln\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_bias_kv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthermal_drop_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m modality_trunks[ModalityType\u001b[38;5;241m.\u001b[39mIMU] \u001b[38;5;241m=\u001b[39m instantiate_trunk(\n\u001b[1;32m    358\u001b[0m     imu_embed_dim,\n\u001b[1;32m    359\u001b[0m     imu_num_blocks,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    363\u001b[0m     drop_path\u001b[38;5;241m=\u001b[39mimu_drop_path,\n\u001b[1;32m    364\u001b[0m )\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nn\u001b[38;5;241m.\u001b[39mModuleDict(modality_trunks)\n",
      "File \u001b[0;32m/ImageBind/imagebind/models/imagebind_model.py:295\u001b[0m, in \u001b[0;36mImageBindModel._create_modality_trunks.<locals>.instantiate_trunk\u001b[0;34m(embed_dim, num_blocks, num_heads, pre_transformer_ln, add_bias_kv, drop_path)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minstantiate_trunk\u001b[39m(\n\u001b[1;32m    293\u001b[0m     embed_dim, num_blocks, num_heads, pre_transformer_ln, add_bias_kv, drop_path\n\u001b[1;32m    294\u001b[0m ):\n\u001b[0;32m--> 295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSimpleTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_blocks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_blocks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mffn_dropout_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdrop_path_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_target\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m            \u001b[49m\u001b[43mMultiheadAttention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m            \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m            \u001b[49m\u001b[43madd_bias_kv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_bias_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpre_transformer_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLayerNorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-6\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpre_transformer_ln\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIdentity\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m            \u001b[49m\u001b[43mEinOpsRearrange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mb l d -> l b d\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_transformer_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEinOpsRearrange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ml b d -> b l d\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ImageBind/imagebind/models/transformer.py:212\u001b[0m, in \u001b[0;36mSimpleTransformer.__init__\u001b[0;34m(self, attn_target, embed_dim, num_blocks, block, pre_transformer_layer, post_transformer_layer, drop_path_rate, drop_path_type, norm_layer, mlp_ratio, ffn_dropout_rate, layer_scale_type, layer_scale_init_value, weight_init_style)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown drop_path_type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdrop_path_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;241m*\u001b[39m[\n\u001b[1;32m    213\u001b[0m         block(\n\u001b[1;32m    214\u001b[0m             dim\u001b[38;5;241m=\u001b[39membed_dim,\n\u001b[1;32m    215\u001b[0m             attn_target\u001b[38;5;241m=\u001b[39mattn_target,\n\u001b[1;32m    216\u001b[0m             mlp_ratio\u001b[38;5;241m=\u001b[39mmlp_ratio,\n\u001b[1;32m    217\u001b[0m             ffn_dropout_rate\u001b[38;5;241m=\u001b[39mffn_dropout_rate,\n\u001b[1;32m    218\u001b[0m             drop_path\u001b[38;5;241m=\u001b[39mdpr[i],\n\u001b[1;32m    219\u001b[0m             norm_layer\u001b[38;5;241m=\u001b[39mnorm_layer,\n\u001b[1;32m    220\u001b[0m             layer_scale_type\u001b[38;5;241m=\u001b[39mlayer_scale_type,\n\u001b[1;32m    221\u001b[0m             layer_scale_init_value\u001b[38;5;241m=\u001b[39mlayer_scale_init_value,\n\u001b[1;32m    222\u001b[0m         )\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_blocks)\n\u001b[1;32m    224\u001b[0m     ]\n\u001b[1;32m    225\u001b[0m )\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_transformer_layer \u001b[38;5;241m=\u001b[39m post_transformer_layer\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_init_style \u001b[38;5;241m=\u001b[39m weight_init_style\n",
      "File \u001b[0;32m/ImageBind/imagebind/models/transformer.py:213\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown drop_path_type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdrop_path_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;241m*\u001b[39m[\n\u001b[0;32m--> 213\u001b[0m         \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattn_target\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmlp_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmlp_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m            \u001b[49m\u001b[43mffn_dropout_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mffn_dropout_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdrop_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdpr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnorm_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlayer_scale_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_scale_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlayer_scale_init_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_scale_init_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_blocks)\n\u001b[1;32m    224\u001b[0m     ]\n\u001b[1;32m    225\u001b[0m )\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_transformer_layer \u001b[38;5;241m=\u001b[39m post_transformer_layer\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_init_style \u001b[38;5;241m=\u001b[39m weight_init_style\n",
      "File \u001b[0;32m/ImageBind/imagebind/models/transformer.py:123\u001b[0m, in \u001b[0;36mBlockWithMasking.__init__\u001b[0;34m(self, dim, attn_target, mlp_ratio, act_layer, norm_layer, ffn_dropout_rate, drop_path, layer_scale_type, layer_scale_init_value)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    121\u001b[0m     attn_target, nn\u001b[38;5;241m.\u001b[39mModule\n\u001b[1;32m    122\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattn_target should be a Callable. Otherwise attn_target is shared across blocks!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn \u001b[38;5;241m=\u001b[39m \u001b[43mattn_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m drop_path \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path \u001b[38;5;241m=\u001b[39m DropPath(drop_path)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/activation.py:987\u001b[0m, in \u001b[0;36mMultiheadAttention.__init__\u001b[0;34m(self, embed_dim, num_heads, dropout, bias, add_bias_kv, add_zero_attn, kdim, vdim, batch_first, device, dtype)\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_zero_attn \u001b[38;5;241m=\u001b[39m add_zero_attn\n\u001b[0;32m--> 987\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/activation.py:991\u001b[0m, in \u001b[0;36mMultiheadAttention._reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_reset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    990\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qkv_same_embed_dim:\n\u001b[0;32m--> 991\u001b[0m         \u001b[43mxavier_uniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    993\u001b[0m         xavier_uniform_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj_weight)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/init.py:327\u001b[0m, in \u001b[0;36mxavier_uniform_\u001b[0;34m(tensor, gain)\u001b[0m\n\u001b[1;32m    324\u001b[0m std \u001b[38;5;241m=\u001b[39m gain \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(fan_in \u001b[38;5;241m+\u001b[39m fan_out))\n\u001b[1;32m    325\u001b[0m a \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m3.0\u001b[39m) \u001b[38;5;241m*\u001b[39m std  \u001b[38;5;66;03m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[0;32m--> 327\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_no_grad_uniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/init.py:14\u001b[0m, in \u001b[0;36m_no_grad_uniform_\u001b[0;34m(tensor, a, b)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_no_grad_uniform_\u001b[39m(tensor, a, b):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 14\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os, torch, gc\n",
    "from tqdm import tqdm\n",
    "from imagebind.models import imagebind_model\n",
    "from imagebind import data\n",
    "\n",
    "\n",
    "# --------------------------------------------\n",
    "# 提取单个样本 embedding（模型即时加载/卸载）\n",
    "# --------------------------------------------\n",
    "def extract_embedding(text_path=None, audio_path=None, video_path=None):\n",
    "    #  每次都加载模型（不会占用太久显存）\n",
    "    model = imagebind_model.imagebind_huge(pretrained=False).to(device)\n",
    "    weight_path = \"/ImageBind/model.safetensors\"\n",
    "    state_dict = torch.load(weight_path, map_location=device)\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    model.eval()\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "        \n",
    "    inputs = {}\n",
    "\n",
    "    if text_path and os.path.exists(text_path):\n",
    "        with open(text_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read().strip()\n",
    "        if len(text) > 0:\n",
    "            inputs[imagebind_model.ModalityType.TEXT] = data.load_and_transform_text([text], device)\n",
    "\n",
    "    if audio_path and os.path.exists(audio_path):\n",
    "        inputs[imagebind_model.ModalityType.AUDIO] = data.load_and_transform_audio_data([audio_path], device)\n",
    "\n",
    "    if video_path and os.path.exists(video_path):\n",
    "        inputs[imagebind_model.ModalityType.VISION] = data.load_and_transform_vision_data(\n",
    "            [video_path], device, num_frames=2\n",
    "        )\n",
    "\n",
    "    if len(inputs) == 0:\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        return None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embedding = model(inputs)\n",
    "\n",
    "    embed_list = [v for v in embedding.values()]\n",
    "    fused = torch.stack(embed_list).mean(dim=0)\n",
    "\n",
    "    #  清理显存\n",
    "    del model, embedding, inputs\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return fused.cpu()\n",
    "\n",
    "\n",
    "# --------------------------------------------\n",
    "# 遍历每个 Session\n",
    "# --------------------------------------------\n",
    "sessions = [f\"Session{i}\" for i in range(1, 6)]\n",
    "\n",
    "for session in sessions:\n",
    "    print(f\"\\n Processing {session} ...\")\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    session_path = os.path.join(BASE_PATH, session, session)\n",
    "    dialog_path = os.path.join(session_path, \"dialog\")\n",
    "\n",
    "    wav_path = os.path.join(dialog_path, \"wav\")\n",
    "    text_path = os.path.join(dialog_path, \"transcriptions\")\n",
    "    avi_path = os.path.join(dialog_path, \"avi\", \"DIVX\")\n",
    "\n",
    "    if not os.path.exists(wav_path):\n",
    "        print(f\" Missing wav directory in {session}\")\n",
    "        continue\n",
    "\n",
    "    embeddings = {}\n",
    "\n",
    "    for root, _, files in os.walk(wav_path):\n",
    "        for file in tqdm(files, desc=f\"{session} audio files\"):\n",
    "            if not is_valid_file(file) or not file.endswith(\".wav\"):\n",
    "                continue\n",
    "\n",
    "            base_id = os.path.splitext(file)[0]\n",
    "            wav_file = os.path.join(root, file)\n",
    "            txt_file = os.path.join(text_path, f\"{base_id}.txt\")\n",
    "            avi_file = os.path.join(avi_path, f\"{base_id}.avi\")\n",
    "\n",
    "            try:\n",
    "                emb = extract_embedding(text_path=txt_file, audio_path=wav_file, video_path=avi_file)\n",
    "            except Exception as e:\n",
    "                print(f\" Error processing {base_id}: {e}\")\n",
    "                continue\n",
    "\n",
    "            if emb is None:\n",
    "                continue\n",
    "\n",
    "            embeddings[base_id] = {\n",
    "                \"text_path\": txt_file if os.path.exists(txt_file) else None,\n",
    "                \"audio_path\": wav_file,\n",
    "                \"video_path\": avi_file if os.path.exists(avi_file) else None,\n",
    "                \"embedding\": emb.squeeze(0),\n",
    "            }\n",
    "\n",
    "            #  每 5 条保存一次中间结果\n",
    "            if len(embeddings) % 5 == 0:\n",
    "                torch.save(embeddings, os.path.join(OUTPUT_DIR, f\"{session}_partial.pt\"))\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "    out_path = os.path.join(OUTPUT_DIR, f\"{session}_embeddings.pt\")\n",
    "    torch.save(embeddings, out_path)\n",
    "    print(f\" Saved {len(embeddings)} embeddings to {out_path}\")\n",
    "\n",
    "    del embeddings\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n All sessions processed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587c8db8-612f-45c8-bb7e-723422bfeaef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
